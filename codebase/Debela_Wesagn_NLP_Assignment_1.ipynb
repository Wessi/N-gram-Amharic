{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP) - Assignment 1\n",
    "## Instructor:\n",
    "* **Fantahun B. (Ph.D.)**\n",
    "### Student:\n",
    "* **Name:** Wesagn Dawit\n",
    "* **ID:** GSR/5257/15\n",
    "* **Program:** MSc in Artificial Intelligence\n",
    "#### Introduction:\n",
    "* This individual assignment is based on one version of the General Purpose Amharic Corpus (GPAC). The assignment involved building **n-gram language models for n values of n = 1,2,3,4** and evaluating their performance **intrinsically and extrinsically**. For intrinsic evaluation, **perplexity** was calculated by determining the probabilities of n-grams in the corpus and generating random sentences based on these n-grams. Since the corpus lacked labels, **text generation** served as the extrinsic evaluation method. Extrinsically, the models' ability to accurately assign probabilities to new sequences was assessed by calculating the likelihoods of test sentences under each model. Extrinsic evaluation utilizes the language models to generate text, with quality assessment based on probability. Higher probabilities indicate better generalization as the model was more likely to generate that sentence. Given an unlabeled corpus, text generation was employed to evaluate the n-gram models, creating random sentences for n=1 to 4. The generated sentences were evaluated based on their coherence and ability to make sense. The higher the n-gram model, the more coherent the generated sentence is. However, the higher the n-gram model, the more data is required to train the model. Therefore, the n-gram model should be chosen based on the available data and We think that is why our instructor specifically asked us to create n-gram models for n values of n = 1,2,3,4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:39.752348600Z",
     "start_time": "2023-12-07T11:05:39.746344300Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:40.640595200Z",
     "start_time": "2023-12-07T11:05:39.756353700Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the GPAC.txt file line by line and store the lines directly into a list.\n",
    "start = time.time()\n",
    "with open('sampleGPAC.txt', 'r') as f:\n",
    "    data = f.readlines()[0]\n",
    "\n",
    "# # add ኢትዮጵያ ታሪካዊ ሀገር ናት to the data\n",
    "data += \" \" + \"ኢትዮጵያ ታሪካዊ ሀገር ናት የህዝብ የድጋፍ ድም ወሳኝ መሆኑን የተገነዘበው ወዳጄ ነው እንግዲህ\"+data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:40.655566100Z",
     "start_time": "2023-12-07T11:05:40.642568300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in origianl corpus:  18720061\n"
     ]
    }
   ],
   "source": [
    "# get the length of the list\n",
    "print(\"Total number of tokens in origianl corpus: \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:40.748646700Z",
     "start_time": "2023-12-07T11:05:40.658568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      " ምን መሰላችሁ? (አንባቢያን) ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው ያልቻለችው የአለም የእግር ኳስ ዋ ለ19ኛ ጊዜ በደቡብ አፍሪካ ሲጠጣ፣ በሩቅ እያየች አንጀቷ ባረረ ልክ በአመቱ በለስ ቀናትና ሌላ ዋ ልትታደም ሁለት ልጆቿን ወደ ደቡብ አፍሪካ ላከች፡፡6ኛው ቢግ ብራዘርስ አፍሪካ አብሮ የመኖር ውድድር በደቡብ አፍሪካ ተካሂዷል፡፡ ከተለያዩ 14 የአፍሪካ አገራት የተውጣጡ 26 ያህል ተሳታፊዎች የተካፈሉበት ይህ ውድድር፣ ግለሰቦች በፈታኝ ሁኔታ ውስጥ በማለፍ ብቃታቸውን የሚያስመሰክሩበት መሆኑን ሰምተናል፡፡ የሚገጥሟቸውን የተለያዩ ፈተናዎች በትእግስትና በጥበብ ማለፍ፣ ከሌሎች ጋር ተስማምቶ መዝለቅ፣ ችግሮችን በብልጠት መፍታት ወዘተ     በየጊዜው ከሚደረገው ቅነሳ ተርፈው ለ91 ቀናት ያህል በውድድሩ መቆየት የቻሉ ሁለት ተወዳዳሪዎች እያንዳንዳቸው 200 ሺህ ዶላር እንደሚሸለሙም\n"
     ]
    }
   ],
   "source": [
    "# print sample 1000 characters\n",
    "print(\"Sample data:\\n\", data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "#### 1. Tokenize the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:40.748646700Z",
     "start_time": "2023-12-07T11:05:40.677569300Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a list of punctuation marks\n",
    "special_chars = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "amharic_chars = ['፡፡', \"::\", '፡', '።', '፣', '፤', '፥', '፦', '፧', '፨']\n",
    "\n",
    "geez = ['፩', '፪', '፫', '፬', '፭', '፮', '፯', '፰', '፱', '፲', '፳', '፴', '፵', '፶', '፷', '፸', '፹', '፺', '፻']\n",
    "\n",
    "puncs = list(set(special_chars + amharic_chars + geez))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:40.749669700Z",
     "start_time": "2023-12-07T11:05:40.692643600Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to process the tokens\n",
    "def processed_list(split_list: list):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(split_list):\n",
    "        if (split_list[i] == ':' and i + 1 < len(split_list) and split_list[i + 1] == ':') or \\\n",
    "                (split_list[i] == '፡' and i + 1 < len(split_list) and split_list[i + 1] == '፡'):\n",
    "            tokens.append('።')\n",
    "            i += 2  # Skip the next character as it is part of a consecutive pair\n",
    "        else:\n",
    "            tokens.append(split_list[i])\n",
    "            i += 1\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:40.749669700Z",
     "start_time": "2023-12-07T11:05:40.707643500Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to tokenize the text data\n",
    "def amh_tokenizer(text: str):\n",
    "    # Escape punctuations to ensure they are not interpreted as regex operators\n",
    "    escaped_puncs = [re.escape(p) for p in puncs]\n",
    "\n",
    "    # Create the regex pattern: words or any of the specified punctuations\n",
    "    pattern = r'\\w+|' + '|'.join(escaped_puncs)\n",
    "\n",
    "    # Use re.findall to get all matches\n",
    "    words = re.findall(pattern, text)\n",
    "\n",
    "    # Remove empty strings and items with space only from the list\n",
    "    split_list = [word for word in words if word != '' and word != ' ']\n",
    "\n",
    "    return processed_list(split_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "411"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the original data\n",
    "tokenized_data = amh_tokenizer(data)\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:44.946434900Z",
     "start_time": "2023-12-07T11:05:43.293436100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  3987374\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens: \", len(tokenized_data))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:45.022470Z",
     "start_time": "2023-12-07T11:05:44.947955100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:45.023461700Z",
     "start_time": "2023-12-07T11:05:44.966435200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['ምን',\n 'መሰላችሁ',\n '?',\n '(',\n 'አንባቢያን',\n ')',\n 'ኢትዮጵያ',\n 'በተደጋጋሚ',\n 'ጥሪው',\n 'ደርሷት',\n 'ልትታደመው',\n 'ያልቻለችው',\n 'የአለም',\n 'የእግር',\n 'ኳስ',\n 'ዋ',\n 'ለ19ኛ',\n 'ጊዜ',\n 'በደቡብ',\n 'አፍሪካ',\n 'ሲጠጣ',\n '፣',\n 'በሩቅ',\n 'እያየች',\n 'አንጀቷ',\n 'ባረረ',\n 'ልክ',\n 'በአመቱ',\n 'በለስ',\n 'ቀናትና',\n 'ሌላ',\n 'ዋ',\n 'ልትታደም',\n 'ሁለት',\n 'ልጆቿን',\n 'ወደ',\n 'ደቡብ',\n 'አፍሪካ',\n 'ላከች',\n '፡፡',\n '6ኛው',\n 'ቢግ',\n 'ብራዘርስ',\n 'አፍሪካ',\n 'አብሮ',\n 'የመኖር',\n 'ውድድር',\n 'በደቡብ',\n 'አፍሪካ',\n 'ተካሂዷል',\n '፡፡',\n 'ከተለያዩ',\n '14',\n 'የአፍሪካ',\n 'አገራት',\n 'የተውጣጡ',\n '26',\n 'ያህል',\n 'ተሳታፊዎች',\n 'የተካፈሉበት',\n 'ይህ',\n 'ውድድር',\n '፣',\n 'ግለሰቦች',\n 'በፈታኝ',\n 'ሁኔታ',\n 'ውስጥ',\n 'በማለፍ',\n 'ብቃታቸውን',\n 'የሚያስመሰክሩበት',\n 'መሆኑን',\n 'ሰምተናል',\n '፡፡',\n 'የሚገጥሟቸውን',\n 'የተለያዩ',\n 'ፈተናዎች',\n 'በትእግስትና',\n 'በጥበብ',\n 'ማለፍ',\n '፣',\n 'ከሌሎች',\n 'ጋር',\n 'ተስማምቶ',\n 'መዝለቅ',\n '፣',\n 'ችግሮችን',\n 'በብልጠት',\n 'መፍታት',\n 'ወዘተ',\n 'በየጊዜው',\n 'ከሚደረገው',\n 'ቅነሳ',\n 'ተርፈው',\n 'ለ91',\n 'ቀናት',\n 'ያህል',\n 'በውድድሩ',\n 'መቆየት',\n 'የቻሉ',\n 'ሁለት',\n 'ተወዳዳሪዎች',\n 'እያንዳንዳቸው',\n '200',\n 'ሺህ',\n 'ዶላር',\n 'እንደሚሸለሙም',\n 'ሲናገር',\n 'ነበር',\n '፡፡',\n 'በዘንድሮው',\n 'ውድድር',\n 'አገራችን',\n 'ዳኒ',\n 'እና',\n 'ሃኒ',\n 'የተባሉ',\n 'ሁለት',\n 'ወጣቶችን',\n 'ብታሰልፍም',\n 'ዳኒ',\n 'ቀደም',\n 'ብሎ',\n 'የቅነሳው',\n 'ሰለባ',\n 'ሲሆን',\n 'ሃኒም',\n 'በቅርቡ',\n 'ከውድድር',\n 'ውጭ',\n 'ሆናለች',\n '፡፡',\n 'ይህቺን',\n 'የአገሪቱ',\n 'ብቸኛ',\n 'ተስፋ',\n 'ወደ',\n 'አሸናፊነት',\n 'ለማሸጋገር',\n 'የህዝብ',\n 'የድጋፍ',\n 'ድም',\n 'ወሳኝ',\n 'መሆኑን',\n 'የተገነዘበው',\n 'ወዳጄ',\n 'ነው',\n 'እንግዲህ',\n '835',\n 'የሚል',\n 'አገራዊ',\n 'ጥሪ',\n 'ያስተላለፈልኝ',\n 'ያኔ',\n 'ሃኒ',\n 'ከውድድሩ',\n 'ከመሰናበቷ',\n 'በፊት',\n '፡፡',\n 'ወዳጄ',\n 'የአገሩን',\n 'ስም',\n 'በአሸናፊነት',\n 'የማስጠራት',\n 'ከፍተኛ',\n 'ጉጉት',\n '፣',\n 'አገሬ',\n 'እንዳትሸነፍ',\n 'የሚል',\n 'ከፍተኛ',\n 'ስጋት',\n 'እንዳደረበት',\n 'ይሰማኛል',\n '፡፡',\n 'ጉጉቱ',\n 'ሳይሆን',\n 'ስጋቱ',\n 'የወዳጄን',\n 'የዋህነት',\n '፡፡',\n 'ሃኒም',\n 'ኢትዮጵያም',\n 'ይሸነፉ',\n 'ይሆን',\n '?',\n 'በሚል',\n 'እንዲህ',\n 'ከንቱ',\n 'ስጋት',\n 'የሚያንገበግባቸውን',\n 'አገር',\n 'ወዳድ',\n 'ዜጐች',\n 'እኔ',\n 'የዋሆች',\n 'እላቸዋለሁ',\n '፡፡',\n 'የዋሆች',\n 'ሆይ',\n '!',\n 'አትስጉ',\n 'ስለ',\n 'ሃኒም',\n 'ስለ',\n 'ኢትዮጵያም',\n 'አትስጉ',\n '፡፡',\n 'ውድድሩ',\n 'ቢግ',\n 'ብራዘርስ',\n 'አፍሪካ',\n 'በአርቴፊሻል',\n 'ፈተናዎች',\n 'ውስጥ',\n 'አልፎ',\n 'ለሦስት',\n 'ወራት',\n 'የመቆየት',\n 'ውድድር',\n 'ነው',\n '፡፡',\n 'ደቡብ',\n 'አፍሪካ',\n 'ለስድስተኛ',\n 'ጊዜ',\n 'ካዘጋጀችው',\n 'ቢግ',\n 'ብራዘርስ',\n 'የመረረ',\n 'ውድድር',\n 'ለሺህ',\n 'አመታት',\n 'በተከታታይ',\n 'ስታዘጋጅ',\n 'የኖረች',\n '፣',\n 'መላ',\n 'ህዝቧን',\n 'አሳትፋ',\n 'መላ',\n 'ህዝቧን',\n 'ስትሸልም',\n 'የኖረች',\n 'አገር',\n 'ናት',\n 'ኢትዮጵያ',\n '!',\n 'የደቡብ',\n 'አፍሪካው',\n 'እንጂ',\n 'የኢትዮጵያው',\n 'ቢግ',\n 'ብራዘርስ',\n 'ሶስት',\n 'ወር',\n 'ተብሎ',\n 'ቀን',\n 'የሚቆረጥለት',\n 'ውስን',\n 'የፈተና',\n 'ጊዜ',\n 'የለውም',\n '፡፡',\n 'አንዲት',\n 'ኢትዮጵያ',\n 'በአንዲት',\n 'ሃኒ',\n 'ሳይሆን',\n 'በመላው',\n 'ህዝቧ',\n 'ነው',\n 'የምትወከለው',\n 'ለምን',\n 'ቢባል',\n 'ሶስት',\n 'ወር',\n 'ሳይሆን',\n 'ሶስት',\n 'ሺህ',\n 'ዘመን',\n 'በራሷ',\n 'ቢግ',\n 'ብራዘርስ',\n 'ተካፍላ',\n 'የምንጊዜም',\n 'አሸናፊ',\n 'ሆና',\n 'ዘልቃለችና',\n '፡፡',\n 'ኢትዮጵያ',\n 'ለዘመናት',\n 'ባስተናገደችው',\n 'የራሷ',\n 'ቢግ',\n 'ብራዘርስ',\n 'አቻ',\n 'የለሽ',\n 'ፈተና',\n 'ውስጥ',\n 'መላው',\n 'ህዝቧን',\n 'እያሳተፈች',\n 'ድሉን',\n 'ከህዝቧ',\n 'እጅ',\n 'ባለማስነጠቅ',\n 'ሃትሪክ',\n 'የሰራች',\n '(',\n 'በሺህ',\n 'አመታት',\n 'ስሌት',\n ')',\n 'የምንጊዜም',\n 'ድል',\n 'ባለቤት',\n 'እኮ',\n 'ናት',\n '!',\n 'የዋሆች',\n 'ስለ',\n 'ደቡብ',\n 'አፍሪካው',\n 'ቢግ',\n 'ብራዘር',\n 'ስለምን',\n 'ትጨነቃላችሁ',\n '?',\n 'ሃኒ',\n 'እኮ',\n 'ለእግር',\n 'ኳስ',\n 'አይደለም',\n 'የሄደችው',\n '፡፡',\n 'እሱንማ',\n 'ብለነው',\n 'ብለነው',\n 'አልሆን',\n 'ብሎ',\n 'ቸግሮናል',\n '፡፡',\n 'ደረጃችን',\n 'ከሌሎች',\n 'በታች',\n 'ሆኖ',\n 'ቀርቶብን',\n 'በሩቅ',\n 'እያየነው',\n 'ተብሰልስለናል',\n '፡፡',\n 'አሁን',\n 'ሃኒን',\n 'ወደ',\n 'ደቡብ',\n 'አፍሪካ',\n 'የላክናት',\n 'ከአቅሟ',\n 'በላይ',\n 'ሳይሆን',\n 'በታች',\n 'ወርዳ',\n 'ወደምትጫወትበት',\n 'አብሮ',\n 'የመኖር',\n 'ቀላል',\n 'ፉክክር',\n 'ነው',\n '፡፡',\n 'ሰቆቃንና',\n 'ፈተናን',\n 'ተጋፍጦ',\n 'በመኖር',\n 'የአለም',\n 'ሻምፒዮናነቱን',\n 'አለም',\n 'በአንድ',\n 'ድም',\n 'ያፀደቀለት',\n 'አሸናፊ',\n 'ህዝብ',\n 'ወኪል',\n 'የሆነችው',\n 'ሃኒ',\n '፣',\n 'ያለ',\n 'ዲቪዚዮኗ',\n 'ስንትና',\n 'ስንት',\n 'ቁልቁል',\n 'ወርዳ',\n 'እኮ',\n 'ነው',\n 'የተወዳደረችው',\n '፡፡',\n 'ሃኒ',\n 'ከቢግ',\n 'ብራዘር',\n 'ውድድር',\n 'ውጭ',\n 'መሆኗን',\n 'ሰሞኑን',\n 'ሰማሁ',\n '፡፡',\n 'ሰማሁና',\n 'ሳቅኩ',\n '፡፡',\n 'ለምን',\n 'ሳቅኩ',\n '?',\n 'የአገሬ',\n 'መሸነፍ',\n 'የማያንገበግበኝ',\n 'ሰው',\n 'ሆኜ',\n 'ነውን',\n '?',\n 'አይመስለኝም',\n '!',\n 'ሃኒ',\n 'ከውድድሩ',\n 'ውጭ',\n 'የሆነችው',\n 'የቢግ',\n 'ብራዘርስ',\n 'አብሮ',\n 'የመኖር',\n 'ውድድር',\n 'አሸንፏት',\n 'ወይም',\n 'አቅቷት',\n 'አይመስለኝም',\n '!',\n 'እሷ',\n 'ከመስፈርቱ',\n 'በላይ',\n 'ሆና',\n 'እንጂ',\n '!',\n '(',\n 'እንደ',\n 'ለት',\n ')',\n 'ኢትዮጵያ',\n 'ከ6ኛው',\n 'የቢግ',\n 'ብራዘርስ',\n 'አፍሪካ',\n 'ውድድር',\n 'የተሰናበተችው',\n 'በቀላል',\n 'ሚዛን',\n 'ውድድር',\n 'የከባድ',\n 'ሚዛን',\n 'ተወዳዳሪ',\n 'በማሰለፏ',\n 'ነው',\n 'ባይ',\n 'ነኝ',\n '፡፡',\n 'ልክ',\n 'በእግር',\n 'ኳስ',\n 'ውድድር',\n 'ላይ',\n 'እንደሚከሰተው',\n 'ለምሳሌ',\n 'ከ16',\n 'አመት',\n 'በታች',\n 'የሆኑ',\n 'ተጫዋቾች',\n 'በሚሳተፉበት',\n 'የታዳጊ',\n 'ወጣቶች',\n 'ሻምፒዮና',\n 'ላይ',\n 'የ25',\n 'አመት',\n 'እድሜ',\n 'ያለው',\n 'ተጫዋች',\n 'በማሰለፉ',\n 'ከውድድር',\n 'ውጭ',\n 'እንደሚሆን',\n 'ክለብ',\n '፡፡',\n 'ይመስለኛል',\n 'ሃኒ',\n 'ከውድድሩ',\n 'የተባረረችው',\n 'የቢግ',\n 'ብራዘርስ',\n 'ፈተና',\n 'ስላቃታት',\n 'አይደለም',\n '፡፡',\n 'ምናልባትም',\n 'እሷ',\n 'ለፈተናው',\n 'ከብዳው',\n 'ቢሆን',\n 'እንጂ',\n '፡፡',\n 'ሃኒ',\n 'ከቢግ',\n 'ብራዘር',\n 'ግቢ',\n 'ተባረረች',\n 'የሚሉ',\n '፣',\n 'እነሱ',\n 'አንዷን',\n 'ሃኒ',\n 'ብቻ',\n 'የሚያዩ',\n 'የዋሆች',\n 'ናቸው',\n '፡፡',\n 'ሃኒ',\n 'እዚያው',\n 'ደቡብ',\n 'አፍሪካ',\n 'ናት',\n 'ደቡብ',\n 'አፍሪካ',\n 'ኡጋንዳ',\n 'ኬኒያ',\n 'ሊቢያ',\n 'ሻሸመኔ',\n 'ቤሩት',\n 'አሜሪካ',\n 'እዚህም',\n 'እዚያም',\n 'ተበትና',\n 'ከቢግ',\n 'ብራዘር',\n 'የመረረ',\n 'ሰቆቃ',\n 'ውስጥ',\n 'እየተንገላታች',\n 'ችግር',\n 'ቻይነቷን',\n 'እያስመሰከረች',\n 'ያለች',\n 'ብዙ',\n 'ኢትዮጵያዊ',\n 'ሴት',\n 'ናት',\n 'ሃኒ',\n '፡፡',\n 'ቢግ',\n 'ብራዘር',\n 'በሚሉት',\n 'የፌክ',\n 'ፈተና',\n 'እና',\n 'ፎርጅድ',\n 'ውጣ',\n 'ውረድ',\n 'ተሸነፋችሁ',\n 'ሲሉን',\n 'ሰማሁና',\n 'ሳቅኩ',\n '!',\n 'ዳኒ',\n 'እና',\n 'ሃኒ',\n 'አይችሉም',\n 'ተብለው',\n 'መባረራቸው',\n 'በሳቅ',\n 'አፈረሰኝ',\n '፡፡',\n 'ምን',\n 'ማለታቸው',\n 'ነው',\n 'ዳኞቹ',\n '?',\n 'እንደ',\n 'ዳኒ',\n 'እንደ',\n 'ሃኒ',\n 'እንደ',\n 'መላው',\n 'ኢትዮጵያዊ',\n 'ለችግር',\n 'ሳይረታ',\n 'ዘመናትን',\n 'ያለፈ',\n 'ማን',\n 'ነው',\n '?',\n 'ተቻችሎ',\n 'መኖር',\n 'ከሆነ',\n 'ጉዳዩ',\n 'ማን',\n 'እንደነሱ',\n 'ተቻቻይ',\n 'አለና',\n 'ነው',\n '!',\n 'ስድብ',\n 'ዘለፋን',\n 'አይደለም',\n '፣',\n 'ግርፋትን',\n 'ችሎ',\n 'የኖረ',\n 'ቆዳው',\n 'ድርብ',\n 'ህዝብ',\n 'እኮ',\n 'ነው',\n 'ተሸንፈሃል',\n 'የተባለው',\n '፡፡',\n 'ከዚህ',\n 'በላይ',\n 'ኮሜዲ',\n 'አለ',\n 'እንዴ',\n '?',\n 'ለሶስት',\n 'ሺህ',\n 'ዘመን',\n 'ክፉ',\n 'ደጉን',\n 'ችሎ',\n 'አብሮ',\n 'የኖረ',\n 'ህዝብ',\n 'እንዴት',\n 'ነው',\n 'ለሶስት',\n 'ወር',\n 'አብሮ',\n 'መኖር',\n 'አቃተህ',\n 'ተብሎ',\n 'ቀይ',\n 'ካርድ',\n 'የሚሰጠው',\n '?',\n 'ሃኒ',\n 'እና',\n 'ዳኒ',\n 'እኮ',\n 'በቢግ',\n 'ቢግ',\n 'ቢግ',\n 'ብራዘርስ',\n 'ኢትዮጵያ',\n 'ፈታኝ',\n 'የኑሮ',\n 'ውድድር',\n 'ለዘመናት',\n 'አሸናፊ',\n 'የሆነው',\n 'የመላው',\n 'ኢትዮጵያዊ',\n 'ወኪሎች',\n 'ናቸው',\n '፡፡',\n 'ትከሻው',\n 'መከራን',\n 'መሸከም',\n 'የማይደክመው',\n '፣',\n 'በስቃይ',\n 'ውስጥም',\n 'ተቻችሎ',\n 'የሚኖረው',\n 'ኢትዮጵያዊ',\n 'ከሴት',\n 'ሃኒ',\n '፣',\n 'ከወንድ',\n 'ዳኒ',\n 'ብሎ',\n 'የወከላቸው',\n 'ናቸው',\n '፡፡',\n 'ሁለቱን',\n 'ከቢግ',\n 'ብራዘር',\n 'አፍሪካ',\n 'ውድድር',\n 'ውጭ',\n 'ማድረግ',\n 'የመላውን',\n 'ችግር',\n 'ቻይና',\n 'አብሮ',\n 'ኗሪ',\n 'ህዝብ',\n 'ክብር',\n 'መንካት',\n 'ነው',\n '፡፡',\n 'ቢግ',\n 'ብራዘርስ',\n 'አፍሪካ',\n 'እንጂ',\n 'አመቱን',\n 'ጠብቆ',\n 'የሚካሄደው',\n '፣',\n 'ቢግ',\n 'ብራዘርስ',\n 'ኢትዮጵያ',\n 'ይሄው',\n 'ከዓመት',\n 'አመት',\n 'እየተካሄደ',\n 'ይገኛል',\n '፡፡',\n 'በየጓዳ',\n 'ጐድጓዳው',\n 'የሚካሄደውን',\n 'የእኛን',\n 'የቀን',\n 'ተቀን',\n 'የመኖር',\n 'ውድድር',\n 'የሚቀር',\n 'ካሜራ',\n 'ባይጠመድም',\n 'ፍልሚያው',\n 'ይሄው',\n 'ተጧጡፎ',\n 'ቀጥሏል',\n '፡፡',\n 'በእነሱ',\n 'እንጂ',\n 'በእኛ',\n 'ቢግ',\n 'ብራዘር',\n 'ለመወዳደር',\n 'ምዝገባ',\n 'አያስፈልግም',\n '፡፡',\n 'ማጣሪያውን',\n 'ለማለፍ',\n 'የህዝብ',\n 'ድም',\n 'ወሳኝ',\n 'አይደለም',\n '፡፡',\n 'የ200',\n 'ሺህ',\n 'ዶላር',\n 'ሽልማት',\n 'ባገኝ',\n 'ብሎ',\n 'አይደለም',\n 'ኢትዮጵያዊ',\n 'ወደ',\n 'አገሩ',\n 'ቢግ',\n 'ብራዘር',\n 'የሚገባው',\n '፡፡',\n 'የደቡብ',\n 'አፍሪካውን',\n 'እንጂ',\n 'የእኛን',\n 'ቢግ',\n 'ብራዘር',\n 'የውድድር',\n 'ሂደት',\n 'የሚቀርፀው',\n 'ካሜራ',\n 'በተወሰነ',\n 'ቦታ',\n 'ላይ',\n 'አይተከልም',\n '፡፡',\n 'ደፋ',\n 'ቀና',\n 'ስንል',\n 'የሚያሳየንን',\n '፣',\n 'ቻይነታችንን',\n 'የሚያመለክተውን',\n 'የኑሮ',\n 'ፊልማችንን',\n 'የአለም',\n 'አይን',\n 'ለዘመናት',\n 'ደጋግሞ',\n 'ሲያየው',\n 'ኖሯል',\n 'እያየም',\n 'ሲያደንቀን',\n '፣',\n 'ሲንቀን',\n '፣',\n 'ሲስቅብን',\n '፣',\n 'ሲሳለቅብን',\n 'ሙድ',\n 'ሲይዝብን',\n '፡፡',\n 'ተሸነፋችሁ',\n 'የተባልንበት',\n 'አርቴፊሻል',\n 'ቢግ',\n 'ብራዘር',\n 'አፍሪካ',\n 'ከመጀመሩ',\n 'ከዘመናት',\n 'በፊት',\n 'እኮ',\n 'ነው',\n 'የእኛው',\n 'ብሔራዊ',\n 'ቢግ',\n 'ብራዘር',\n 'የተጀመረው',\n '፡፡',\n 'ከቢግ',\n 'ብራዘር',\n 'አፍሪካ',\n 'ቀድሞ',\n 'አለም',\n 'የእኛን',\n 'ቢግ',\n 'ብራዘር',\n 'ይከታተል',\n 'ነበር',\n '፡፡',\n 'አርቴፊሻል',\n 'ያልሆነውን',\n 'ፈተናችንን',\n '፣',\n 'ተሰልቶ',\n 'ያልተሰጠንን',\n 'መከራችንን',\n '፣',\n 'ህግ',\n 'ያልወጣለት',\n 'ገደብ',\n 'ያልተቀመጠለት',\n 'ከእዚህ',\n 'እስከ',\n 'እዚያ',\n 'ያልተባለለት',\n 'አብሮ',\n 'የመኖር',\n 'ትራጀዲያችንን',\n 'እያየ',\n 'አለም',\n 'ሁሉ',\n 'አጃኢብ',\n 'ሲል',\n 'ኖሯል',\n '፡፡',\n 'ተደናቂው',\n 'የእኛ',\n 'ቢግ',\n 'ብራዘር',\n 'በተወሰነ',\n 'ቻናል',\n 'ለተወሰነ',\n 'ተመልካች',\n 'አይደለም',\n 'ሲሰራጭ',\n 'የኖረው',\n '፡፡',\n 'አለም',\n 'ሁሉ',\n 'ሲያየው',\n 'ኖሯል',\n 'ዲኤስ',\n 'ቲቪ',\n 'ሳያስገጥም',\n '!',\n 'ያየውንም',\n 'ፎ',\n 'አስቀምጦታል',\n 'በየታሪክ',\n 'ድርሳኑ',\n 'በየ',\n 'መዝገበ',\n 'ቃላቱ',\n '፡፡',\n '(',\n 'በነገራችን',\n 'ላይ',\n ')',\n 'እነ',\n 'ሃኒ',\n 'ያሸንፉ',\n 'ዘንድ',\n 'የህዝብ',\n 'ድም',\n 'እንደ',\n 'ሁሉ',\n 'በኦክስፎርድ',\n 'ላይ',\n 'ያለውን',\n 'የሚል',\n 'ቃል',\n 'ፍቺ',\n 'ለማሠረዝም',\n 'የህዝብ',\n 'ድም',\n 'ዋጋ',\n 'የለውም',\n '!',\n '(',\n 'በፌስ',\n 'ቡክ',\n 'በኦክስፎርድ',\n 'መዝገበ',\n 'ቃላት',\n 'ላይ',\n 'ስለ',\n 'ኢትዮጵያ',\n 'የተፃፈውን',\n 'ፀያፍ',\n 'ነገር',\n 'ለማሰረዝ',\n 'ድም',\n 'እንስጥ',\n 'የሚል',\n 'ዘመቻ',\n 'መጀመሩን',\n 'ልብ',\n 'በሉ',\n ')',\n 'እና',\n 'ከኦክስፎርድ',\n 'ማሰረዝ',\n 'ቢቻል',\n 'እንኳን',\n 'ከአለም',\n 'ልቡና',\n 'ግን',\n 'መፋቅ',\n 'አይቻልም',\n '፡፡',\n 'እና',\n '!',\n 'ሃኒ',\n 'በ',\n 'ቢግ',\n 'ብራዘርስ',\n 'አፍሪካ',\n 'አልተሸነፈችም',\n '!',\n 'ምናልባት',\n 'ሃኒ',\n 'በ',\n 'ቢግ',\n 'ቢራዘርስ',\n 'አልቀረም',\n '፡፡',\n 'ሀኪሞቹ',\n 'ብልሀቱ',\n 'ገባቸው',\n '፡፡',\n 'ውሀ',\n 'ፈልቶ',\n 'በቂ',\n 'ይንተከተካል',\n '፡፡',\n '/',\n 'ውስጡ',\n 'ሊኖር',\n 'የሚችለው',\n 'በሽታ',\n 'እንዲጠፋ',\n '/',\n 'ውሀውን',\n 'በመርፌ',\n 'መድሀኒት',\n 'አስመስለው',\n 'ይወጉዋቸውና',\n '፣',\n 'ኋየታዘዘልህን',\n 'ኪኒን',\n 'እንደተባልከው',\n 'ካልዋጥክ',\n 'ግን',\n 'መርፌው',\n 'ብቻውን',\n 'አይሰራም',\n '፣',\n 'ይረክሳል',\n 'ይሉዋቸዋል',\n '፡፡',\n '/',\n 'ይሄ',\n '/',\n 'ስነ']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 1000 tokens\n",
    "tokenized_data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove unnecessary characters from the tokenized data\n",
    "* **Purpose:** Simplify corpus, reduce noise, enhance generalization\n",
    "* **Approach:** Exclude punctuation marks to focus on essential content\n",
    "* **Considerations:** Task-specific requirements may warrant preserving certain punctuation **e.g: keep '::', '?'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:48.841810400Z",
     "start_time": "2023-12-07T11:05:44.995440100Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove punctuation marks from the tokenized data\n",
    "def remove_puncs(tokens: list):\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in puncs or token == '።' or token == '፡፡' or token == '?':\n",
    "            processed_tokens.append(token)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "processed_tokenized_data = remove_puncs(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "32"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tokenized_data\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:48.952449Z",
     "start_time": "2023-12-07T11:05:48.842973Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:49.029549900Z",
     "start_time": "2023-12-07T11:05:48.955450600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens after unecessary characters:  3757118\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens after unecessary characters: \", len(processed_tokenized_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Normalize the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:05:49.037511400Z",
     "start_time": "2023-12-07T11:05:48.971415900Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize the tokens by replacing vague character or sequence\n",
    "def normalize(norm: str):\n",
    "    replacements = {\"ሃ\": \"ሀ\", \"ኅ\": \"ሀ\", \"ኃ\": \"ሀ\", \"ሐ\": \"ሀ\", \"ሓ\": \"ሀ\", \"ኻ\": \"ሀ\", \"ሑ\": \"ሁ\",\n",
    "                    \"ኁ\": \"ሁ\", \"ዅ\": \"ሁ\", \"ኂ\": \"ሂ\", \"ሒ\": \"ሂ\", \"ኺ\": \"ሂ\", \"ኌ\": \"ሄ\", \"ሔ\": \"ሄ\",\n",
    "                    \"ዄ\": \"ሄ\", \"ሕ\": \"ህ\", \"ኆ\": \"ሆ\", \"ሖ\": \"ሆ\", \"ኾ\": \"ሆ\", \"ሠ\": \"ሰ\", \"ሡ\": \"ሱ\",\n",
    "                    \"ሢ\": \"ሲ\", \"ሣ\": \"ሳ\", \"ሤ\": \"ሴ\", \"ሥ\": \"ስ\", \"ሦ\": \"ሶ\", \"ዓ\": \"አ\", \"ኣ\": \"አ\",\n",
    "                    \"ዐ\": \"አ\", \"ዑ\": \"ኡ\", \"ዒ\": \"ኢ\", \"ዔ\": \"ኤ\", \"ዕ\": \"እ\", \"ዖ\": \"ኦ\", \"ፀ\": \"ጸ\",\n",
    "                    \"ፁ\": \"ጹ\", \"ጺ\": \"ፂ\", \"ጻ\": \"ፃ\", \"ጼ\": \"ፄ\", \"ፅ\": \"ጽ\", \"ፆ\": \"ጾ\"}\n",
    "\n",
    "    for character, replacement in replacements.items():\n",
    "        norm = norm.replace(character, replacement)\n",
    "\n",
    "    specific_patterns = [\n",
    "        '(ሉ[ዋአሃ])', '(ሙ[ዋአሃ])', '(ቱ[ዋአሃ])', '(ሩ[ዋአሃ])', '(ሱ[ዋአሃ])', '(ሹ[ዋአሃ])', '(ቁ[ዋአሃ])',\n",
    "        '(ቡ[ዋአሃ])', '(ቹ[ዋአሃ])', '(ሁ[ዋአሃ])', '(ኑ[ዋአሃ])', '(ኙ[ዋአሃ])', '(ኩ[ዋአሃ])', '(ዙ[ዋአሃ])',\n",
    "        '(ጉ[ዋአሃ])', '(ደ[ዋአሃ])', '(ጡ[ዋአሃ])', '(ጩ[ዋአሃ])', '(ጹ[ዋአሃ])', '(ፉ[ዋአሃ])', '[ቊ]', '[ኵ]',\n",
    "        '\\s+'\n",
    "    ]\n",
    "    replacements_specific = [\n",
    "        'ሏ', 'ሟ', 'ቷ', 'ሯ', 'ሷ', 'ሿ', 'ቋ',\n",
    "        'ቧ', 'ቿ', 'ኋ', 'ኗ', 'ኟ', 'ኳ', 'ዟ',\n",
    "        'ጓ', 'ዷ', 'ጧ', 'ጯ', 'ጿ', 'ፏ', 'ቁ', 'ኩ',\n",
    "        ' '\n",
    "    ]\n",
    "\n",
    "    for pattern, replacement in zip(specific_patterns, replacements_specific):\n",
    "        norm = re.sub(pattern, replacement, norm)\n",
    "\n",
    "\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# normalized_data = [normalize(token) for token in processed_tokenized_data]\n",
    "normalized_data = []\n",
    "\n",
    "for i in range(len(processed_tokenized_data)):\n",
    "    token = processed_tokenized_data[i]\n",
    "    if token in [\"።\", \"፡፡\"]:\n",
    "        normalized_data.append('።')\n",
    "    # For all other tokens, apply the normalize function\n",
    "    else:\n",
    "        normalized_data.append(normalize(token))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:34.607236Z",
     "start_time": "2023-12-07T11:05:49.046515200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens after normalization:  3757118\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens after normalization: \", len(normalized_data))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:34.623236600Z",
     "start_time": "2023-12-07T11:07:34.609237200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:34.728244100Z",
     "start_time": "2023-12-07T11:07:34.625242500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_tokenized\t|normalized\n",
      "----------------------------------------\n",
      "፡፡                        ።                        \n",
      "፡፡                        ።                        \n",
      "፡፡                        ።                        \n"
     ]
    }
   ],
   "source": [
    "# print processed_tokenized_data vs normalized_data\n",
    "print(\"processed_tokenized\\t|normalized\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(100):\n",
    "    if processed_tokenized_data[i] != normalized_data[i]:\n",
    "        print(\"{:25} {:25}\".format(processed_tokenized_data[i], normalized_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "64"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del processed_tokenized_data\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:34.798237Z",
     "start_time": "2023-12-07T11:07:34.646241400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1. N-gram language model\n",
    "### 1.1. Create n-grams for n=1, 2, 3, 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:34.820248Z",
     "start_time": "2023-12-07T11:07:34.797237300Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to create n-grams\n",
    "def create_n_grams(tokens: list, n: int):\n",
    "\n",
    "    # filtered_tokens = [token for token in tokens if token not in ['<s>', '</s><s>', '</q><s>']]\n",
    "    filtered_tokens = [token for token in tokens]\n",
    "\n",
    "    # Now, create n-grams from the filtered_tokens list\n",
    "    return [filtered_tokens[i:i + n] for i in range(len(filtered_tokens) - n + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Unigrams (n=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:38.682240400Z",
     "start_time": "2023-12-07T11:07:34.824243500Z"
    }
   },
   "outputs": [],
   "source": [
    "# create n-grams for n=1\n",
    "unigrams = create_n_grams(normalized_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:38.700239800Z",
     "start_time": "2023-12-07T11:07:38.684236600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": "[['ምን'],\n ['መሰላችሁ'],\n ['አንባቢያን'],\n ['ኢትዮጵያ'],\n ['በተደጋጋሚ'],\n ['ጥሪው'],\n ['ደርሷት'],\n ['ልትታደመው'],\n ['ያልቻለችው'],\n ['የአለም']]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print 10 unigrams (skip punctuation marks)\n",
    "print(\"Unigrams:\")\n",
    "([unigram for unigram in unigrams[:100] if unigram[0] not in puncs][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bigrams (n=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:41.814590400Z",
     "start_time": "2023-12-07T11:07:38.701240900Z"
    }
   },
   "outputs": [],
   "source": [
    "# create n-grams for n=2\n",
    "bigrams = create_n_grams(normalized_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:41.858591400Z",
     "start_time": "2023-12-07T11:07:41.815591100Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Print 10 bigrams (skip punctuation marks)\n",
    "# print(\"Bigrams:\")\n",
    "# ([bigram for bigram in bigrams[:100] if bigram[0] not in puncs and bigram[1] not in puncs][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Trigrams (n=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:45.231723500Z",
     "start_time": "2023-12-07T11:07:41.837596900Z"
    }
   },
   "outputs": [],
   "source": [
    "# create n-grams for n=3\n",
    "trigrams = create_n_grams(normalized_data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:45.232724200Z",
     "start_time": "2023-12-07T11:07:45.220721900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": "[['አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ'],\n ['ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው'],\n ['በተደጋጋሚ', 'ጥሪው', 'ደርሷት'],\n ['ጥሪው', 'ደርሷት', 'ልትታደመው'],\n ['ደርሷት', 'ልትታደመው', 'ያልቻለችው'],\n ['ልትታደመው', 'ያልቻለችው', 'የአለም'],\n ['ያልቻለችው', 'የአለም', 'የእግር'],\n ['የአለም', 'የእግር', 'ኳስ'],\n ['የእግር', 'ኳስ', 'ዋ'],\n ['ኳስ', 'ዋ', 'ለ19ኛ']]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Print 10 trigrams (skip punctuation marks)\n",
    "print(\"Trigrams:\")\n",
    "([trigram for trigram in trigrams[:100] if trigram[0] not in puncs and trigram[1] not in puncs and trigram[2] not in puncs][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fourgrams (n=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:47.907671400Z",
     "start_time": "2023-12-07T11:07:45.234725Z"
    }
   },
   "outputs": [],
   "source": [
    "# create n-grams for n=4\n",
    "fourgrams = create_n_grams(normalized_data, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:47.933677600Z",
     "start_time": "2023-12-07T11:07:47.912670100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourgrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": "[['አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው'],\n ['ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት'],\n ['በተደጋጋሚ', 'ጥሪው', 'ደርሷት', 'ልትታደመው'],\n ['ጥሪው', 'ደርሷት', 'ልትታደመው', 'ያልቻለችው'],\n ['ደርሷት', 'ልትታደመው', 'ያልቻለችው', 'የአለም'],\n ['ልትታደመው', 'ያልቻለችው', 'የአለም', 'የእግር'],\n ['ያልቻለችው', 'የአለም', 'የእግር', 'ኳስ'],\n ['የአለም', 'የእግር', 'ኳስ', 'ዋ'],\n ['የእግር', 'ኳስ', 'ዋ', 'ለ19ኛ'],\n ['ኳስ', 'ዋ', 'ለ19ኛ', 'ጊዜ']]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Print 10 fourgrams (skip punctuation marks)\n",
    "print(\"Fourgrams:\")\n",
    "([fourgram for fourgram in fourgrams[:100] if fourgram[0] not in puncs and fourgram[1] not in puncs and fourgram[2] not in puncs and fourgram[3] not in puncs][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n.\n",
    "#### Probability of n-grams (n=1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def calculate_n_gram_probabilities(n_grams: list):\n",
    "    \"\"\"Calculate n-gram probabilities.\"\"\"\n",
    "    n_gram_counts = Counter(tuple(n_gram) for n_gram in n_grams)\n",
    "    total_n_grams = sum(n_gram_counts.values())\n",
    "\n",
    "    n_gram_probabilities = {n_gram: count / total_n_grams for n_gram, count in n_gram_counts.items()}\n",
    "    return n_gram_probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:48.022665600Z",
     "start_time": "2023-12-07T11:07:47.942668600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Calculate probabilities for each n-gram size\n",
    "unigram_probabilities = calculate_n_gram_probabilities(unigrams)\n",
    "bigram_probabilities = calculate_n_gram_probabilities(bigrams)\n",
    "trigram_probabilities = calculate_n_gram_probabilities(trigrams)\n",
    "fourgram_probabilities = calculate_n_gram_probabilities(fourgrams)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:53.777600700Z",
     "start_time": "2023-12-07T11:07:47.967668600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Top 10 most likely n-grams (n=1, 2, 3, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m \u001B[92m \n",
      "Top 5 most likely unigrams \u001B[00m\n",
      "----------------------------------------\n",
      "ነው                   0.01494789\n",
      "ቢግ                   0.01046334\n",
      "ሀኒ                   0.00946683\n"
     ]
    }
   ],
   "source": [
    "# print the first 5 probabilities for all n\n",
    "\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m\".format(\"\\nTop 5 most likely unigrams\"))\n",
    "print(\"-\" * 40)\n",
    "for unigram, prob in sorted(unigram_probabilities.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    if unigram[0] not in puncs:\n",
    "        print(\"{:20} {:10.8f}\".format(unigram[0], prob))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:53.794601300Z",
     "start_time": "2023-12-07T11:07:53.781603100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m \u001B[92m \n",
      "Top 10 most likely bigrams \u001B[00m\n",
      "----------------------------------------\n",
      "ቢግ ብራዘርስ             0.00498254\n",
      "ቢግ ብራዘር              0.00448429\n",
      "ደቡብ አፍሪካ             0.00249127\n",
      "ብራዘርስ አፍሪካ           0.00249127\n",
      "ከቢግ ብራዘር             0.00249127\n",
      "አብሮ የመኖር             0.00199302\n"
     ]
    }
   ],
   "source": [
    "# print top 10 most likely bi-grams\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m\".format(\"\\nTop 10 most likely bigrams\"))\n",
    "print(\"-\" * 40)\n",
    "for bigram, prob in sorted(bigram_probabilities.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    if bigram[0] not in puncs and bigram[1] not in puncs:\n",
    "        print(\"{:20} {:10.8f}\".format(bigram[0] + \" \" + bigram[1], prob))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:53.886627200Z",
     "start_time": "2023-12-07T11:07:53.796600700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m \u001B[92m \n",
      "Top 10 most likely trigrams \u001B[00m\n",
      "----------------------------------------\n",
      "ቢግ ብራዘርስ አፍሪካ        0.00199302\n",
      "ወደ ደቡብ አፍሪካ          0.00099651\n",
      "አብሮ የመኖር ውድድር        0.00099651\n",
      "ዳኒ እና ሀኒ             0.00099651\n",
      "ሀኒ ከቢግ ብራዘር          0.00099651\n",
      "ቢግ ብራዘርስ ኢትዮጵያ       0.00099651\n",
      "ከቢግ ብራዘር አፍሪካ        0.00099651\n",
      "የእኛን ቢግ ብራዘር         0.00099651\n"
     ]
    }
   ],
   "source": [
    "# print top 10 most likely tri-grams\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m\".format(\"\\nTop 10 most likely trigrams\"))\n",
    "print(\"-\" * 40)\n",
    "for trigram, prob in sorted(trigram_probabilities.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    if trigram[0] not in puncs and trigram[1] not in puncs and trigram[2] not in puncs:\n",
    "        print(\"{:20} {:10.8f}\".format(trigram[0] + \" \" + trigram[1] + \" \" + trigram[2], prob))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:53.898627900Z",
     "start_time": "2023-12-07T11:07:53.812601100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m \u001B[92m \n",
      "Top 10 most likely fourgrams \u001B[00m\n",
      "----------------------------------------\n",
      "የህዝብ የድጋፍ ድም ወሳኝ     0.00049852\n",
      "የድጋፍ ድም ወሳኝ መሆኑን     0.00049852\n",
      "ድም ወሳኝ መሆኑን የተገነዘበው  0.00049852\n",
      "ወሳኝ መሆኑን የተገነዘበው ወዳጄ 0.00049852\n",
      "መሆኑን የተገነዘበው ወዳጄ ነው  0.00049852\n",
      "አንባቢያን ኢትዮጵያ በተደጋጋሚ ጥሪው 0.00049825\n",
      "ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት 0.00049825\n"
     ]
    }
   ],
   "source": [
    "# print top 10 most likely four-grams\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m\".format(\"\\nTop 10 most likely fourgrams\"))\n",
    "print(\"-\" * 40)\n",
    "for fourgram, prob in sorted(fourgram_probabilities.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    if fourgram[0] not in puncs and fourgram[1] not in puncs and fourgram[2] not in puncs and fourgram[3] not in puncs:\n",
    "        print(\"{:20} {:10.8f}\".format(fourgram[0] + \" \" + fourgram[1] + \" \" + fourgram[2] + \" \" + fourgram[3], prob))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:07:53.899628600Z",
     "start_time": "2023-12-07T11:07:53.829604100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Probability of a given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "2.6616166925952493e-07"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the probability of a given sentence using four-grams\n",
    "sentence = normalize(\"ኢትዮጵያ ታሪካዊ ሀገር ናት\")\n",
    "fourgram_probabilities[tuple(sentence.split())]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:11:33.695847600Z",
     "start_time": "2023-12-07T11:11:33.672385600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:11:36.271090400Z",
     "start_time": "2023-12-07T11:11:36.239070500Z"
    }
   },
   "outputs": [],
   "source": [
    "# find the probability of a given sentence\n",
    "def sentence_probability(sentence, n, unigram_probabilities):\n",
    "    words = sentence.split()\n",
    "    total_probability = 1.0\n",
    "\n",
    "    if n == 1:\n",
    "        n_grams = unigram_probabilities\n",
    "        sentence_grams = [(words[i],) for i in range(len(words))]  # Single word tuples\n",
    "    elif n == 2:\n",
    "        n_grams = bigram_probabilities\n",
    "        sentence_grams = [(words[i], words[i+1]) for i in range(len(words) - 1)]\n",
    "    elif n == 3:\n",
    "        n_grams = trigram_probabilities\n",
    "        sentence_grams = [(words[i], words[i+1], words[i+2]) for i in range(len(words) - 2)]\n",
    "    elif n == 4:\n",
    "        n_grams = fourgram_probabilities\n",
    "        sentence_grams = [(words[i], words[i+1], words[i+2], words[i+3]) for i in range(len(words) - 3)]\n",
    "    else:\n",
    "        raise ValueError(\"n must be between 1 and 4\")\n",
    "\n",
    "    for gram in sentence_grams:\n",
    "        prob = n_grams.get(gram, 1e-10)  # Get the probability, default to 0.0000000001 if not found\n",
    "        total_probability *= prob\n",
    "\n",
    "    return total_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:22:26.764059800Z",
     "start_time": "2023-12-07T11:22:26.740032700Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the probability of a sentence for sentence\n",
    "sentence = \"ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው\" #ኢትዮጵያ ታሪካዊ ሀገር ናት\n",
    "uingtram_probability = sentence_probability(sentence, 1, unigram_probabilities)\n",
    "bigram_probability = sentence_probability(sentence, 2, bigram_probabilities)\n",
    "trigram_probability = sentence_probability(sentence, 3, trigram_probabilities)\n",
    "fourgram_probability = sentence_probability(sentence, 4, fourgram_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram probability of sentence 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው': 2.4568239180391327e-16\n",
      "Bigram probability of sentence 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው': 6.163174996726693e-14\n",
      "Trigram probability of sentence 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው': 1.2369544895145099e-10\n",
      "Fourgram probability of sentence 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው': 2.482576911184169e-07\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unigram probability of sentence '{sentence}': {uingtram_probability}\")\n",
    "print(f\"Bigram probability of sentence '{sentence}': {bigram_probability}\")\n",
    "print(f\"Trigram probability of sentence '{sentence}': {trigram_probability}\")\n",
    "print(f\"Fourgram probability of sentence '{sentence}': {fourgram_probability}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:22:27.477800600Z",
     "start_time": "2023-12-07T11:22:27.427803100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4568239180391327e-16, 6.163174996726693e-14, 1.2369544895145099e-10, 2.482576911184169e-07]\n"
     ]
    }
   ],
   "source": [
    "# sort\n",
    "sorted = [uingtram_probability, bigram_probability, trigram_probability, fourgram_probability]\n",
    "sorted.sort()\n",
    "print(sorted)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:30:20.947195300Z",
     "start_time": "2023-12-07T11:30:20.896013700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4. Generate random sentences using the n-grams for n=1, 2, 3, 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:33:40.926827900Z",
     "start_time": "2023-12-07T11:33:40.878129800Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate random sentences using the n-grams for n=1, 2, 3, 4\n",
    "def generate_random_sentence(n_gram_probabilities):\n",
    "    sentence_end_markers = ['።', '?']\n",
    "    sentence = []\n",
    "    non_marker_added = False  # Flag to check if at least one non-marker word has been added\n",
    "\n",
    "    while True:\n",
    "        word_tuple = random.choices(list(n_gram_probabilities.keys()), weights=n_gram_probabilities.values())[0]\n",
    "        joined_word = []\n",
    "        end_marker = \"\"\n",
    "        end_marker_flag = False\n",
    "\n",
    "        for word in word_tuple:\n",
    "\n",
    "            # if word in sentence_end_markers:\n",
    "            #     print(word)\n",
    "\n",
    "            if word in sentence_end_markers:\n",
    "                end_marker = word\n",
    "                end_marker_flag = True\n",
    "                break\n",
    "            else:\n",
    "                joined_word.append(word)\n",
    "                non_marker_added = True\n",
    "\n",
    "        if end_marker_flag:\n",
    "            sentence.append(' '.join(joined_word))\n",
    "            sentence.append(end_marker)\n",
    "            break\n",
    "        else:\n",
    "            sentence.append(' '.join(joined_word))\n",
    "\n",
    "    if not non_marker_added:\n",
    "        return generate_random_sentence(n_gram_probabilities)\n",
    "\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m \u001B[92m \n",
      "Unigram sentence: \n",
      " \u001B[00m ከትውልድ ብራዘር መጠንቀቅ ልቦለድ ንብረት ይይዙን ቅቃ የባሰ ከተረጨበት እዚች በፊት ደስታ ከፍተኛ ስሙ እንጂ ጨብጦ የአገሬ ይስጡይ ልዩነት እምኖረው እንጂ ወር  ።\n",
      "\u001B[1m \u001B[92m \n",
      "Bigram sentence: \n",
      " \u001B[00m ጨዋታ ይሆናል ዳኒ ብሎ ሶስት ወር ተጨማሪ ምስራዊ የአገሬ መሸነፍ በሚታይ ጊዜ ሰለባ ሲሆን መልካም መረጃ አሮፕላን በሚታይ ስጋ ሁለት ኮንጐ የምትባለው በነገራችን ላይ የሚታየው እብደት ናት ሀኒ ቻይነታችንን የሚያመለክተውን ሊበጠስ ሲል እየደከመ ሄዶ ላይ የአባለ ተጋፍጦ በመኖር እንደሚሸለሙም ሲናገር ውስጥ አልፎ ለሌላዋ ለማሳየት በህመም ተኮማትሮ ያልወጣለት ገደብ ቢሮው የሚገባው እብደት ምን  ።\n",
      "\u001B[1m \u001B[92m \n",
      "Trigram sentence: \n",
      " \u001B[00m በወንድ ላይ ሲደርሱ ሀምሳ አመት በፊት ናቸው ።\n",
      "\u001B[1m \u001B[92m \n",
      "Fourgram sentence: \n",
      " \u001B[00m ሾፌሩንስ ?\n"
     ]
    }
   ],
   "source": [
    "generated_sentence = generate_random_sentence(unigram_probabilities)\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m {}\".format(\"\\nUnigram sentence: \\n\", generated_sentence))\n",
    "\n",
    "generated_sentence = generate_random_sentence(bigram_probabilities)\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m {}\".format(\"\\nBigram sentence: \\n\", generated_sentence))\n",
    "\n",
    "generated_sentence = generate_random_sentence(trigram_probabilities)\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m {}\".format(\"\\nTrigram sentence: \\n\", generated_sentence))\n",
    "\n",
    "generated_sentence = generate_random_sentence(fourgram_probabilities)\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m {}\".format(\"\\nFourgram sentence: \\n\", generated_sentence))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:33:43.952864Z",
     "start_time": "2023-12-07T11:33:43.921101100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "* **Unigram sentence:** The unigram sentence is not coherent and does not make sense. This is because the unigram model does not consider the context of the words. It only considers the probability of each word in the sentence. Therefore, the unigram model is not suitable for generating sentences.\n",
    "* **Bigram sentence:** The bigram sentence is more coherent than the unigram sentence. This is because the bigram model considers the probability of each word given the previous word. Therefore, the bigram model is more suitable for generating sentences than the unigram model.\n",
    "* **Trigram sentence:** The trigram sentence is more coherent than the bigram sentence. This is because the trigram model considers the probability of each word given the previous two words. Therefore, the trigram model is more suitable for generating sentences than the bigram model.\n",
    "* **Fourgram sentence:** The fourgram sentence is more coherent than the trigram sentence. This is because the fourgram model considers the probability of each word given the previous three words. Therefore, the fourgram model is more suitable for generating sentences than the trigram model.\n",
    "* **Conclusion:** The higher the n-gram model, the more coherent the generated sentence is. However, the higher the n-gram model, the more data is required to train the model. Therefore, the n-gram model should be chosen based on the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2. Evaluate these Language Models Using Intrinsic Evaluation Method\n",
    "#### Calculate perplexity for n_gram language models for n=1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:33:48.309353500Z",
     "start_time": "2023-12-07T11:33:48.283338100Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to calculate perplexity\n",
    "def calculate_perplexity(ngrams, total_words):\n",
    "\n",
    "    model_entropy = 0.0\n",
    "    for ngram, count in ngrams.items():\n",
    "        prob = count / total_words\n",
    "        model_entropy += -math.log2(prob)\n",
    "\n",
    "    model_entropy /= total_words\n",
    "    return math.pow(2, model_entropy)\n",
    "\n",
    "def evaluate_model(n, tokens):\n",
    "\n",
    "    ngrams = collections.Counter(tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1))\n",
    "    total_words = len(tokens)\n",
    "\n",
    "    return calculate_perplexity(ngrams, total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:33:56.440227300Z",
     "start_time": "2023-12-07T11:33:49.145562100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1, Perplexity: 1.0025172868281529\n",
      "n = 2, Perplexity: 1.0038525829201945\n",
      "n = 3, Perplexity: 1.0040838860753833\n",
      "n = 4, Perplexity: 1.0041206213523801\n"
     ]
    }
   ],
   "source": [
    "# Evaluating n-gram models for n = 1, 2, 3, 4\n",
    "perplexities = []\n",
    "for n in range(1, 5):\n",
    "    perplexity = evaluate_model(n, normalized_data)\n",
    "    perplexities.append(perplexity)\n",
    "\n",
    "# Printing the results\n",
    "for n, perplexity in enumerate(perplexities):\n",
    "    print(f\"n = {n + 1}, Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "* In general, higher n-gram models (such as trigrams, fourgrams) have higher perplexity compared to lower order n-grams (unigrams, bigrams) often indicate potential challenges or limitations in language model performance. Higher perplexity values imply higher uncertainty and poorer predictions made by the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3 Evaluate these Language Models Using Extrinsic Evaluation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:34:09.424970700Z",
     "start_time": "2023-12-07T11:34:09.385964Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_language_model(language_model, sentences, n):\n",
    "    total_probability = 1.0  # we start with a base probability of 1 as multiplying anything by 1 leaves it unchanged\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "\n",
    "        for i in range(n - 1, len(words)):\n",
    "            phrase = tuple(words[i - (n-1):i + 1])\n",
    "\n",
    "            if phrase in language_model:\n",
    "                total_probability *= language_model[phrase]\n",
    "            else:\n",
    "                total_probability *= 1e-6  # or some small probability for unseen n-grams\n",
    "\n",
    "    # return 0 if the total_probability is 1.0\n",
    "    return total_probability if total_probability != 1.0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:34:11.575929300Z",
     "start_time": "2023-12-07T11:34:11.544971200Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences1 = [\"ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው\", \"ኢትዮጵያ ታሪካዊ ሀገር ናት ሀገር ኢትዮጵያ\"]\n",
    "\n",
    "sentences2 = [\"ደርሷትልትታደመውጥሪው \", \"ሀገርኢትዮጵያ ናትታሪካዊ ሀገርኢትዮጵያ\", \"ኢትዮጵያታሪካዊ ሀገርናትየህዝብበተደጋጋሚ\"]\n",
    "\n",
    "# sentences = [\" \".join(normalized_data[-6:])]\n",
    "sentences = \"ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው\"\n",
    "\n",
    "# sentence = \"ወሳኝ መሆኑን የተገነዘበው ወዳጄ ነው እንግዲህ\"\n",
    "eval_unigram_probability = evaluate_language_model(unigram_probabilities, sentences, 1)\n",
    "eval_bigram_probability = evaluate_language_model(bigram_probabilities, sentences, 2)\n",
    "eval_trigram_probability = evaluate_language_model(trigram_probabilities, sentences, 3)\n",
    "eval_fourgram_probability = evaluate_language_model(fourgram_probabilities, sentences, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T11:34:13.258957Z",
     "start_time": "2023-12-07T11:34:13.162959200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The probability of sentences 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው' is to be generated by unigram model is: 1.2326336870261532e-133\n",
      "\n",
      "The probability of sentences 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው' is to be generated by bigram model is: 0\n",
      "\n",
      "The probability of sentences 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው' is to be generated by trigram model is: 0\n",
      "\n",
      "The probability of sentences 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው' is to be generated by fourgram model is: 0\n"
     ]
    }
   ],
   "source": [
    "# print the results\n",
    "print(\"\\nThe probability of sentences '{}' is to be generated by unigram model is: {}\".format(sentences, eval_unigram_probability))\n",
    "\n",
    "print(\"\\nThe probability of sentences '{}' is to be generated by bigram model is: {}\".format(sentences, eval_bigram_probability))\n",
    "#\n",
    "print(\"\\nThe probability of sentences '{}' is to be generated by trigram model is: {}\".format(sentences, eval_trigram_probability))\n",
    "\n",
    "print(\"\\nThe probability of sentences '{}' is to be generated by fourgram model is: {}\".format(sentences, eval_fourgram_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totol time of execution is 1719.3651468753815 seconds.\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "\n",
    "print(\"Totol time of execution is {} seconds.\".format(end-start))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T11:34:19.149505700Z",
     "start_time": "2023-12-07T11:34:19.120507600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explanation:\n",
    "* In general the higher the n-gram model, the more coherent the generated sentence is. We calculated the probabilities of test sentences under each n-gram model. The assignment of probabilities varied depending on the sentence contents and how well it matched patterns in the training data. Some sentences may have received higher probabilities from higher n models, while others could be better predicted by lower n models. No definitive claim can be made that probabilities will consistently increase or decrease with n."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion:\n",
    "* We have dedicated considerable effort to completing this assignment, engaging in discussions with my peers, particularly Debela, to address the various aspects of the tasks. Our collaborative efforts have been beneficial in broadening our understanding and learning significantly throughout this process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
