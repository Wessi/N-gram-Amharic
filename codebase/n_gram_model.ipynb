{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP) - Assignment 1\n",
    "## Instructor:\n",
    "* **Fantahun B. (Ph.D.)**\n",
    "### Student:\n",
    "* **Name:** Wesagn Dawit\n",
    "* **ID:** GSR/5257/15\n",
    "* **Program:** MSc in Artificial Intelligence\n",
    "#### Introduction:\n",
    "* This individual assignment is based on one version of the General Purpose Amharic Corpus (GPAC). The assignment involved building **n-gram language models for n values of n = 1,2,3,4** and evaluating their performance **intrinsically and extrinsically**. For intrinsic evaluation, **perplexity** was calculated by determining the probabilities of n-grams in the corpus and generating random sentences based on these n-grams. Since the corpus lacked labels, **text generation** served as the extrinsic evaluation method. Extrinsically, the models' ability to accurately assign probabilities to new sequences was assessed by calculating the likelihoods of test sentences under each model. Extrinsic evaluation utilizes the language models to generate text, with quality assessment based on probability. Higher probabilities indicate better generalization as the model was more likely to generate that sentence. Given an unlabeled corpus, text generation was employed to evaluate the n-gram models, creating random sentences for n=1 to 4. The generated sentences were evaluated based on their coherence and ability to make sense. The higher the n-gram model, the more coherent the generated sentence is. However, the higher the n-gram model, the more data is required to train the model. Therefore, the n-gram model should be chosen based on the available data and I think that is why our instructor specifically asked us to create n-gram models for n values of n = 1,2,3,4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T20:17:50.177172500Z",
     "start_time": "2023-11-08T20:17:49.381301800Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T20:17:50.592099400Z",
     "start_time": "2023-11-08T20:17:49.424186700Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the GPAC.txt file line by line and store the lines directly into a list.\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with open('GPAC.txt', 'r') as f:\n",
    "    data = [line.strip() for line in f]\n",
    "data = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T20:17:50.616993900Z",
     "start_time": "2023-11-08T20:17:49.630635300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      " ምን መሰላችሁ? (አንባቢያን) ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው ያልቻለችው የአለም የእግር ኳስ ዋ ለ19ኛ ጊዜ በደቡብ አፍሪካ ሲጠጣ፣ በሩቅ እያየች አንጀቷ ባረረ ልክ በአመቱ በለስ ቀናትና ሌላ ዋ ልትታደም ሁለት ልጆቿን ወደ ደቡብ አፍሪካ ላከች፡፡6ኛው ቢግ ብራዘርስ አፍሪካ አብሮ የመኖር ውድድር በደቡብ አፍሪካ ተካሂዷል፡፡ ከተለያዩ 14 የአፍሪካ አገራት የተውጣጡ 26 ያህል ተሳታፊዎች የተካፈሉበት ይህ ውድድር፣ ግለሰቦች በፈታኝ ሁኔታ ውስጥ በማለፍ ብቃታቸውን የሚያስመሰክሩበት መሆኑን ሰምተናል፡፡ የሚገጥሟቸውን የተለያዩ ፈተናዎች በትእግስትና በጥበብ ማለፍ፣ ከሌሎች ጋር ተስማምቶ መዝለቅ፣ ችግሮችን በብልጠት መፍታት ወዘተ     በየጊዜው ከሚደረገው ቅነሳ ተርፈው ለ91 ቀናት ያህል በውድድሩ መቆየት የቻሉ ሁለት ተወዳዳሪዎች እያንዳንዳቸው 200 ሺህ ዶላር እንደሚሸለሙም\n"
     ]
    }
   ],
   "source": [
    "# print sample 1000 characters\n",
    "print(\"Sample data:\\n\", data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "#### 1. Tokenize the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T20:17:50.616993900Z",
     "start_time": "2023-11-08T20:17:49.649596400Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a list of punctuation marks\n",
    "special_chars = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "amharic_chars = ['፡፡', \"::\", '፡', '።', '፣', '፤', '፥', '፦', '፧', '፨']\n",
    "\n",
    "geez = ['፩', '፪', '፫', '፬', '፭', '፮', '፯', '፰', '፱', '፲', '፳', '፴', '፵', '፶', '፷', '፸', '፹', '፺', '፻']\n",
    "\n",
    "puncs = list(set(special_chars + amharic_chars + geez))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T20:17:50.616993900Z",
     "start_time": "2023-11-08T20:17:49.661551300Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to process the tokens\n",
    "def processed_list(split_list: list):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(split_list):\n",
    "        if (split_list[i] == ':' and i + 1 < len(split_list) and split_list[i + 1] == ':') or \\\n",
    "                (split_list[i] == '፡' and i + 1 < len(split_list) and split_list[i + 1] == '፡'):\n",
    "            tokens.append('።')\n",
    "            i += 2  # Skip the next character as it is part of a consecutive pair\n",
    "        else:\n",
    "            tokens.append(split_list[i])\n",
    "            i += 1\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T20:17:50.626967100Z",
     "start_time": "2023-11-08T20:17:49.681499Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to tokenize the text data\n",
    "def amh_tokenizer(text: str):\n",
    "    # Escape punctuations to ensure they are not interpreted as regex operators\n",
    "    escaped_puncs = [re.escape(p) for p in puncs]\n",
    "\n",
    "    # Create the regex pattern: words or any of the specified punctuations\n",
    "    pattern = r'\\w+|' + '|'.join(escaped_puncs)\n",
    "\n",
    "    # Use re.findall to get all matches\n",
    "    words = re.findall(pattern, text)\n",
    "\n",
    "    # Remove empty strings and items with space only from the list\n",
    "    split_list = [word for word in words if word != '' and word != ' ']\n",
    "\n",
    "    return processed_list(split_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T20:17:56.351378500Z",
     "start_time": "2023-11-08T20:17:52.001299500Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize the original data\n",
    "tokenized_data = amh_tokenizer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T20:17:56.418200Z",
     "start_time": "2023-11-08T20:17:56.354381600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ምን',\n",
       " 'መሰላችሁ',\n",
       " '?',\n",
       " '(',\n",
       " 'አንባቢያን',\n",
       " ')',\n",
       " 'ኢትዮጵያ',\n",
       " 'በተደጋጋሚ',\n",
       " 'ጥሪው',\n",
       " 'ደርሷት',\n",
       " 'ልትታደመው',\n",
       " 'ያልቻለችው',\n",
       " 'የአለም',\n",
       " 'የእግር',\n",
       " 'ኳስ',\n",
       " 'ዋ',\n",
       " 'ለ19ኛ',\n",
       " 'ጊዜ',\n",
       " 'በደቡብ',\n",
       " 'አፍሪካ',\n",
       " 'ሲጠጣ',\n",
       " '፣',\n",
       " 'በሩቅ',\n",
       " 'እያየች',\n",
       " 'አንጀቷ',\n",
       " 'ባረረ',\n",
       " 'ልክ',\n",
       " 'በአመቱ',\n",
       " 'በለስ',\n",
       " 'ቀናትና',\n",
       " 'ሌላ',\n",
       " 'ዋ',\n",
       " 'ልትታደም',\n",
       " 'ሁለት',\n",
       " 'ልጆቿን',\n",
       " 'ወደ',\n",
       " 'ደቡብ',\n",
       " 'አፍሪካ',\n",
       " 'ላከች',\n",
       " '፡፡',\n",
       " '6ኛው',\n",
       " 'ቢግ',\n",
       " 'ብራዘርስ',\n",
       " 'አፍሪካ',\n",
       " 'አብሮ',\n",
       " 'የመኖር',\n",
       " 'ውድድር',\n",
       " 'በደቡብ',\n",
       " 'አፍሪካ',\n",
       " 'ተካሂዷል',\n",
       " '፡፡',\n",
       " 'ከተለያዩ',\n",
       " '14',\n",
       " 'የአፍሪካ',\n",
       " 'አገራት',\n",
       " 'የተውጣጡ',\n",
       " '26',\n",
       " 'ያህል',\n",
       " 'ተሳታፊዎች',\n",
       " 'የተካፈሉበት',\n",
       " 'ይህ',\n",
       " 'ውድድር',\n",
       " '፣',\n",
       " 'ግለሰቦች',\n",
       " 'በፈታኝ',\n",
       " 'ሁኔታ',\n",
       " 'ውስጥ',\n",
       " 'በማለፍ',\n",
       " 'ብቃታቸውን',\n",
       " 'የሚያስመሰክሩበት',\n",
       " 'መሆኑን',\n",
       " 'ሰምተናል',\n",
       " '፡፡',\n",
       " 'የሚገጥሟቸውን',\n",
       " 'የተለያዩ',\n",
       " 'ፈተናዎች',\n",
       " 'በትእግስትና',\n",
       " 'በጥበብ',\n",
       " 'ማለፍ',\n",
       " '፣',\n",
       " 'ከሌሎች',\n",
       " 'ጋር',\n",
       " 'ተስማምቶ',\n",
       " 'መዝለቅ',\n",
       " '፣',\n",
       " 'ችግሮችን',\n",
       " 'በብልጠት',\n",
       " 'መፍታት',\n",
       " 'ወዘተ',\n",
       " 'በየጊዜው',\n",
       " 'ከሚደረገው',\n",
       " 'ቅነሳ',\n",
       " 'ተርፈው',\n",
       " 'ለ91',\n",
       " 'ቀናት',\n",
       " 'ያህል',\n",
       " 'በውድድሩ',\n",
       " 'መቆየት',\n",
       " 'የቻሉ',\n",
       " 'ሁለት',\n",
       " 'ተወዳዳሪዎች',\n",
       " 'እያንዳንዳቸው',\n",
       " '200',\n",
       " 'ሺህ',\n",
       " 'ዶላር',\n",
       " 'እንደሚሸለሙም',\n",
       " 'ሲናገር',\n",
       " 'ነበር',\n",
       " '፡፡',\n",
       " 'በዘንድሮው',\n",
       " 'ውድድር',\n",
       " 'አገራችን',\n",
       " 'ዳኒ',\n",
       " 'እና',\n",
       " 'ሃኒ',\n",
       " 'የተባሉ',\n",
       " 'ሁለት',\n",
       " 'ወጣቶችን',\n",
       " 'ብታሰልፍም',\n",
       " 'ዳኒ',\n",
       " 'ቀደም',\n",
       " 'ብሎ',\n",
       " 'የቅነሳው',\n",
       " 'ሰለባ',\n",
       " 'ሲሆን',\n",
       " 'ሃኒም',\n",
       " 'በቅርቡ',\n",
       " 'ከውድድር',\n",
       " 'ውጭ',\n",
       " 'ሆናለች',\n",
       " '፡፡',\n",
       " 'ይህቺን',\n",
       " 'የአገሪቱ',\n",
       " 'ብቸኛ',\n",
       " 'ተስፋ',\n",
       " 'ወደ',\n",
       " 'አሸናፊነት',\n",
       " 'ለማሸጋገር',\n",
       " 'የህዝብ',\n",
       " 'የድጋፍ',\n",
       " 'ድም',\n",
       " 'ወሳኝ',\n",
       " 'መሆኑን',\n",
       " 'የተገነዘበው',\n",
       " 'ወዳጄ',\n",
       " 'ነው',\n",
       " 'እንግዲህ',\n",
       " '835',\n",
       " 'የሚል',\n",
       " 'አገራዊ',\n",
       " 'ጥሪ',\n",
       " 'ያስተላለፈልኝ',\n",
       " 'ያኔ',\n",
       " 'ሃኒ',\n",
       " 'ከውድድሩ',\n",
       " 'ከመሰናበቷ',\n",
       " 'በፊት',\n",
       " '፡፡',\n",
       " 'ወዳጄ',\n",
       " 'የአገሩን',\n",
       " 'ስም',\n",
       " 'በአሸናፊነት',\n",
       " 'የማስጠራት',\n",
       " 'ከፍተኛ',\n",
       " 'ጉጉት',\n",
       " '፣',\n",
       " 'አገሬ',\n",
       " 'እንዳትሸነፍ',\n",
       " 'የሚል',\n",
       " 'ከፍተኛ',\n",
       " 'ስጋት',\n",
       " 'እንዳደረበት',\n",
       " 'ይሰማኛል',\n",
       " '፡፡',\n",
       " 'ጉጉቱ',\n",
       " 'ሳይሆን',\n",
       " 'ስጋቱ',\n",
       " 'የወዳጄን',\n",
       " 'የዋህነት',\n",
       " '፡፡',\n",
       " 'ሃኒም',\n",
       " 'ኢትዮጵያም',\n",
       " 'ይሸነፉ',\n",
       " 'ይሆን',\n",
       " '?',\n",
       " 'በሚል',\n",
       " 'እንዲህ',\n",
       " 'ከንቱ',\n",
       " 'ስጋት',\n",
       " 'የሚያንገበግባቸውን',\n",
       " 'አገር',\n",
       " 'ወዳድ',\n",
       " 'ዜጐች',\n",
       " 'እኔ',\n",
       " 'የዋሆች',\n",
       " 'እላቸዋለሁ',\n",
       " '፡፡',\n",
       " 'የዋሆች',\n",
       " 'ሆይ',\n",
       " '!',\n",
       " 'አትስጉ',\n",
       " 'ስለ',\n",
       " 'ሃኒም',\n",
       " 'ስለ',\n",
       " 'ኢትዮጵያም',\n",
       " 'አትስጉ',\n",
       " '፡፡',\n",
       " 'ውድድሩ',\n",
       " 'ቢግ',\n",
       " 'ብራዘርስ',\n",
       " 'አፍሪካ',\n",
       " 'በአርቴፊሻል',\n",
       " 'ፈተናዎች',\n",
       " 'ውስጥ',\n",
       " 'አልፎ',\n",
       " 'ለሦስት',\n",
       " 'ወራት',\n",
       " 'የመቆየት',\n",
       " 'ውድድር',\n",
       " 'ነው',\n",
       " '፡፡',\n",
       " 'ደቡብ',\n",
       " 'አፍሪካ',\n",
       " 'ለስድስተኛ',\n",
       " 'ጊዜ',\n",
       " 'ካዘጋጀችው',\n",
       " 'ቢግ',\n",
       " 'ብራዘርስ',\n",
       " 'የመረረ',\n",
       " 'ውድድር',\n",
       " 'ለሺህ',\n",
       " 'አመታት',\n",
       " 'በተከታታይ',\n",
       " 'ስታዘጋጅ',\n",
       " 'የኖረች',\n",
       " '፣',\n",
       " 'መላ',\n",
       " 'ህዝቧን',\n",
       " 'አሳትፋ',\n",
       " 'መላ',\n",
       " 'ህዝቧን',\n",
       " 'ስትሸልም',\n",
       " 'የኖረች',\n",
       " 'አገር',\n",
       " 'ናት',\n",
       " 'ኢትዮጵያ',\n",
       " '!',\n",
       " 'የደቡብ',\n",
       " 'አፍሪካው',\n",
       " 'እንጂ',\n",
       " 'የኢትዮጵያው',\n",
       " 'ቢግ',\n",
       " 'ብራዘርስ',\n",
       " 'ሶስት',\n",
       " 'ወር',\n",
       " 'ተብሎ',\n",
       " 'ቀን',\n",
       " 'የሚቆረጥለት',\n",
       " 'ውስን',\n",
       " 'የፈተና',\n",
       " 'ጊዜ',\n",
       " 'የለውም',\n",
       " '፡፡',\n",
       " 'አንዲት',\n",
       " 'ኢትዮጵያ',\n",
       " 'በአንዲት',\n",
       " 'ሃኒ',\n",
       " 'ሳይሆን',\n",
       " 'በመላው',\n",
       " 'ህዝቧ',\n",
       " 'ነው',\n",
       " 'የምትወከለው',\n",
       " 'ለምን',\n",
       " 'ቢባል',\n",
       " 'ሶስት',\n",
       " 'ወር',\n",
       " 'ሳይሆን',\n",
       " 'ሶስት',\n",
       " 'ሺህ',\n",
       " 'ዘመን',\n",
       " 'በራሷ',\n",
       " 'ቢግ',\n",
       " 'ብራዘርስ',\n",
       " 'ተካፍላ',\n",
       " 'የምንጊዜም',\n",
       " 'አሸናፊ',\n",
       " 'ሆና',\n",
       " 'ዘልቃለችና',\n",
       " '፡፡',\n",
       " 'ኢትዮጵያ',\n",
       " 'ለዘመናት',\n",
       " 'ባስተናገደችው',\n",
       " 'የራሷ',\n",
       " 'ቢግ',\n",
       " 'ብራዘርስ',\n",
       " 'አቻ',\n",
       " 'የለሽ',\n",
       " 'ፈተና',\n",
       " 'ውስጥ',\n",
       " 'መላው',\n",
       " 'ህዝቧን',\n",
       " 'እያሳተፈች',\n",
       " 'ድሉን',\n",
       " 'ከህዝቧ',\n",
       " 'እጅ',\n",
       " 'ባለማስነጠቅ',\n",
       " 'ሃትሪክ',\n",
       " 'የሰራች',\n",
       " '(',\n",
       " 'በሺህ',\n",
       " 'አመታት',\n",
       " 'ስሌት',\n",
       " ')',\n",
       " 'የምንጊዜም',\n",
       " 'ድል',\n",
       " 'ባለቤት',\n",
       " 'እኮ',\n",
       " 'ናት',\n",
       " '!',\n",
       " 'የዋሆች',\n",
       " 'ስለ',\n",
       " 'ደቡብ',\n",
       " 'አፍሪካው',\n",
       " 'ቢግ',\n",
       " 'ብራዘር',\n",
       " 'ስለምን',\n",
       " 'ትጨነቃላችሁ',\n",
       " '?',\n",
       " 'ሃኒ',\n",
       " 'እኮ',\n",
       " 'ለእግር',\n",
       " 'ኳስ',\n",
       " 'አይደለም',\n",
       " 'የሄደችው',\n",
       " '፡፡',\n",
       " 'እሱንማ',\n",
       " 'ብለነው',\n",
       " 'ብለነው',\n",
       " 'አልሆን',\n",
       " 'ብሎ',\n",
       " 'ቸግሮናል',\n",
       " '፡፡',\n",
       " 'ደረጃችን',\n",
       " 'ከሌሎች',\n",
       " 'በታች',\n",
       " 'ሆኖ',\n",
       " 'ቀርቶብን',\n",
       " 'በሩቅ',\n",
       " 'እያየነው',\n",
       " 'ተብሰልስለናል',\n",
       " '፡፡',\n",
       " 'አሁን',\n",
       " 'ሃኒን',\n",
       " 'ወደ',\n",
       " 'ደቡብ',\n",
       " 'አፍሪካ',\n",
       " 'የላክናት',\n",
       " 'ከአቅሟ',\n",
       " 'በላይ',\n",
       " 'ሳይሆን',\n",
       " 'በታች',\n",
       " 'ወርዳ',\n",
       " 'ወደምትጫወትበት',\n",
       " 'አብሮ',\n",
       " 'የመኖር',\n",
       " 'ቀላል',\n",
       " 'ፉክክር',\n",
       " 'ነው',\n",
       " '፡፡',\n",
       " 'ሰቆቃንና',\n",
       " 'ፈተናን',\n",
       " 'ተጋፍጦ',\n",
       " 'በመኖር',\n",
       " 'የአለም',\n",
       " 'ሻምፒዮናነቱን',\n",
       " 'አለም',\n",
       " 'በአንድ',\n",
       " 'ድም',\n",
       " 'ያፀደቀለት',\n",
       " 'አሸናፊ',\n",
       " 'ህዝብ',\n",
       " 'ወኪል',\n",
       " 'የሆነችው',\n",
       " 'ሃኒ',\n",
       " '፣',\n",
       " 'ያለ',\n",
       " 'ዲቪዚዮኗ',\n",
       " 'ስንትና',\n",
       " 'ስንት',\n",
       " 'ቁልቁል',\n",
       " 'ወርዳ',\n",
       " 'እኮ',\n",
       " 'ነው',\n",
       " 'የተወዳደረችው',\n",
       " '፡፡',\n",
       " 'ሃኒ',\n",
       " 'ከቢግ',\n",
       " 'ብራዘር',\n",
       " 'ውድድር',\n",
       " 'ውጭ',\n",
       " 'መሆኗን',\n",
       " 'ሰሞኑን',\n",
       " 'ሰማሁ',\n",
       " '፡፡',\n",
       " 'ሰማሁና',\n",
       " 'ሳቅኩ',\n",
       " '፡፡',\n",
       " 'ለምን',\n",
       " 'ሳቅኩ',\n",
       " '?',\n",
       " 'የአገሬ',\n",
       " 'መሸነፍ',\n",
       " 'የማያንገበግበኝ',\n",
       " 'ሰው',\n",
       " 'ሆኜ',\n",
       " 'ነውን',\n",
       " '?',\n",
       " 'አይመስለኝም',\n",
       " '!',\n",
       " 'ሃኒ',\n",
       " 'ከውድድሩ',\n",
       " 'ውጭ',\n",
       " 'የሆነችው',\n",
       " 'የቢግ',\n",
       " 'ብራዘርስ',\n",
       " 'አብሮ',\n",
       " 'የመኖር',\n",
       " 'ውድድር',\n",
       " 'አሸንፏት',\n",
       " 'ወይም',\n",
       " 'አቅቷት',\n",
       " 'አይመስለኝም',\n",
       " '!',\n",
       " 'እሷ',\n",
       " 'ከመስፈርቱ',\n",
       " 'በላይ',\n",
       " 'ሆና',\n",
       " 'እንጂ',\n",
       " '!',\n",
       " '(',\n",
       " 'እንደ',\n",
       " 'ለት',\n",
       " ')',\n",
       " 'ኢትዮጵያ',\n",
       " 'ከ6ኛው',\n",
       " 'የቢግ',\n",
       " 'ብራዘርስ',\n",
       " 'አፍሪካ',\n",
       " 'ውድድር',\n",
       " 'የተሰናበተችው',\n",
       " 'በቀላል',\n",
       " 'ሚዛን',\n",
       " 'ውድድር',\n",
       " 'የከባድ',\n",
       " 'ሚዛን',\n",
       " 'ተወዳዳሪ',\n",
       " 'በማሰለፏ',\n",
       " 'ነው',\n",
       " 'ባይ',\n",
       " 'ነኝ',\n",
       " '፡፡',\n",
       " 'ልክ',\n",
       " 'በእግር',\n",
       " 'ኳስ',\n",
       " 'ውድድር',\n",
       " 'ላይ',\n",
       " 'እንደሚከሰተው',\n",
       " 'ለምሳሌ',\n",
       " 'ከ16',\n",
       " 'አመት',\n",
       " 'በታች',\n",
       " 'የሆኑ',\n",
       " 'ተጫዋቾች',\n",
       " 'በሚሳተፉበት',\n",
       " 'የታዳጊ',\n",
       " 'ወጣቶች',\n",
       " 'ሻምፒዮና',\n",
       " 'ላይ',\n",
       " 'የ25',\n",
       " 'አመት',\n",
       " 'እድሜ',\n",
       " 'ያለው',\n",
       " 'ተጫዋች',\n",
       " 'በማሰለፉ',\n",
       " 'ከውድድር',\n",
       " 'ውጭ',\n",
       " 'እንደሚሆን',\n",
       " 'ክለብ',\n",
       " '፡፡',\n",
       " 'ይመስለኛል',\n",
       " 'ሃኒ',\n",
       " 'ከውድድሩ',\n",
       " 'የተባረረችው',\n",
       " 'የቢግ',\n",
       " 'ብራዘርስ',\n",
       " 'ፈተና',\n",
       " 'ስላቃታት',\n",
       " 'አይደለም',\n",
       " '፡፡',\n",
       " 'ምናልባትም',\n",
       " 'እሷ',\n",
       " 'ለፈተናው',\n",
       " 'ከብዳው',\n",
       " 'ቢሆን',\n",
       " 'እንጂ',\n",
       " '፡፡',\n",
       " 'ሃኒ',\n",
       " 'ከቢግ',\n",
       " 'ብራዘር',\n",
       " 'ግቢ',\n",
       " 'ተባረረች',\n",
       " 'የሚሉ',\n",
       " '፣',\n",
       " 'እነሱ',\n",
       " 'አንዷን',\n",
       " 'ሃኒ',\n",
       " 'ብቻ',\n",
       " 'የሚያዩ',\n",
       " 'የዋሆች',\n",
       " 'ናቸው',\n",
       " '፡፡',\n",
       " 'ሃኒ',\n",
       " 'እዚያው',\n",
       " 'ደቡብ',\n",
       " 'አፍሪካ',\n",
       " 'ናት',\n",
       " 'ደቡብ',\n",
       " 'አፍሪካ',\n",
       " 'ኡጋንዳ',\n",
       " 'ኬኒያ',\n",
       " 'ሊቢያ',\n",
       " 'ሻሸመኔ',\n",
       " 'ቤሩት',\n",
       " 'አሜሪካ',\n",
       " 'እዚህም',\n",
       " 'እዚያም',\n",
       " 'ተበትና',\n",
       " 'ከቢግ',\n",
       " 'ብራዘር',\n",
       " 'የመረረ',\n",
       " 'ሰቆቃ',\n",
       " 'ውስጥ',\n",
       " 'እየተንገላታች',\n",
       " 'ችግር',\n",
       " 'ቻይነቷን',\n",
       " 'እያስመሰከረች',\n",
       " 'ያለች',\n",
       " 'ብዙ',\n",
       " 'ኢትዮጵያዊ',\n",
       " 'ሴት',\n",
       " 'ናት',\n",
       " 'ሃኒ',\n",
       " '፡፡',\n",
       " 'ቢግ',\n",
       " 'ብራዘር',\n",
       " 'በሚሉት',\n",
       " 'የፌክ',\n",
       " 'ፈተና',\n",
       " 'እና',\n",
       " 'ፎርጅድ',\n",
       " 'ውጣ',\n",
       " 'ውረድ',\n",
       " 'ተሸነፋችሁ',\n",
       " 'ሲሉን',\n",
       " 'ሰማሁና',\n",
       " 'ሳቅኩ',\n",
       " '!',\n",
       " 'ዳኒ',\n",
       " 'እና',\n",
       " 'ሃኒ',\n",
       " 'አይችሉም',\n",
       " 'ተብለው',\n",
       " 'መባረራቸው',\n",
       " 'በሳቅ',\n",
       " 'አፈረሰኝ',\n",
       " '፡፡',\n",
       " 'ምን',\n",
       " 'ማለታቸው',\n",
       " 'ነው',\n",
       " 'ዳኞቹ',\n",
       " '?',\n",
       " 'እንደ',\n",
       " 'ዳኒ',\n",
       " 'እንደ',\n",
       " 'ሃኒ',\n",
       " 'እንደ',\n",
       " 'መላው',\n",
       " 'ኢትዮጵያዊ',\n",
       " 'ለችግር',\n",
       " 'ሳይረታ',\n",
       " 'ዘመናትን',\n",
       " 'ያለፈ',\n",
       " 'ማን',\n",
       " 'ነው',\n",
       " '?',\n",
       " 'ተቻችሎ',\n",
       " 'መኖር',\n",
       " 'ከሆነ',\n",
       " 'ጉዳዩ',\n",
       " 'ማን',\n",
       " 'እንደነሱ',\n",
       " 'ተቻቻይ',\n",
       " 'አለና',\n",
       " 'ነው',\n",
       " '!',\n",
       " 'ስድብ',\n",
       " 'ዘለፋን',\n",
       " 'አይደለም',\n",
       " '፣',\n",
       " 'ግርፋትን',\n",
       " 'ችሎ',\n",
       " 'የኖረ',\n",
       " 'ቆዳው',\n",
       " 'ድርብ',\n",
       " 'ህዝብ',\n",
       " 'እኮ',\n",
       " 'ነው',\n",
       " 'ተሸንፈሃል',\n",
       " 'የተባለው',\n",
       " '፡፡',\n",
       " 'ከዚህ',\n",
       " 'በላይ',\n",
       " 'ኮሜዲ',\n",
       " 'አለ',\n",
       " 'እንዴ',\n",
       " '?',\n",
       " 'ለሶስት',\n",
       " 'ሺህ',\n",
       " 'ዘመን',\n",
       " 'ክፉ',\n",
       " 'ደጉን',\n",
       " 'ችሎ',\n",
       " 'አብሮ',\n",
       " 'የኖረ',\n",
       " 'ህዝብ',\n",
       " 'እንዴት',\n",
       " 'ነው',\n",
       " 'ለሶስት',\n",
       " 'ወር',\n",
       " 'አብሮ',\n",
       " 'መኖር',\n",
       " 'አቃተህ',\n",
       " 'ተብሎ',\n",
       " 'ቀይ',\n",
       " 'ካርድ',\n",
       " 'የሚሰጠው',\n",
       " '?',\n",
       " 'ሃኒ',\n",
       " 'እና',\n",
       " 'ዳኒ',\n",
       " 'እኮ',\n",
       " 'በቢግ',\n",
       " 'ቢግ',\n",
       " 'ቢግ',\n",
       " 'ብራዘርስ',\n",
       " 'ኢትዮጵያ',\n",
       " 'ፈታኝ',\n",
       " 'የኑሮ',\n",
       " 'ውድድር',\n",
       " 'ለዘመናት',\n",
       " 'አሸናፊ',\n",
       " 'የሆነው',\n",
       " 'የመላው',\n",
       " 'ኢትዮጵያዊ',\n",
       " 'ወኪሎች',\n",
       " 'ናቸው',\n",
       " '፡፡',\n",
       " 'ትከሻው',\n",
       " 'መከራን',\n",
       " 'መሸከም',\n",
       " 'የማይደክመው',\n",
       " '፣',\n",
       " 'በስቃይ',\n",
       " 'ውስጥም',\n",
       " 'ተቻችሎ',\n",
       " 'የሚኖረው',\n",
       " 'ኢትዮጵያዊ',\n",
       " 'ከሴት',\n",
       " 'ሃኒ',\n",
       " '፣',\n",
       " 'ከወንድ',\n",
       " 'ዳኒ',\n",
       " 'ብሎ',\n",
       " 'የወከላቸው',\n",
       " 'ናቸው',\n",
       " '፡፡',\n",
       " 'ሁለቱን',\n",
       " 'ከቢግ',\n",
       " 'ብራዘር',\n",
       " 'አፍሪካ',\n",
       " 'ውድድር',\n",
       " 'ውጭ',\n",
       " 'ማድረግ',\n",
       " 'የመላውን',\n",
       " 'ችግር',\n",
       " 'ቻይና',\n",
       " 'አብሮ',\n",
       " 'ኗሪ',\n",
       " 'ህዝብ',\n",
       " 'ክብር',\n",
       " 'መንካት',\n",
       " 'ነው',\n",
       " '፡፡',\n",
       " 'ቢግ',\n",
       " 'ብራዘርስ',\n",
       " 'አፍሪካ',\n",
       " 'እንጂ',\n",
       " 'አመቱን',\n",
       " 'ጠብቆ',\n",
       " 'የሚካሄደው',\n",
       " '፣',\n",
       " 'ቢግ',\n",
       " 'ብራዘርስ',\n",
       " 'ኢትዮጵያ',\n",
       " 'ይሄው',\n",
       " 'ከዓመት',\n",
       " 'አመት',\n",
       " 'እየተካሄደ',\n",
       " 'ይገኛል',\n",
       " '፡፡',\n",
       " 'በየጓዳ',\n",
       " 'ጐድጓዳው',\n",
       " 'የሚካሄደውን',\n",
       " 'የእኛን',\n",
       " 'የቀን',\n",
       " 'ተቀን',\n",
       " 'የመኖር',\n",
       " 'ውድድር',\n",
       " 'የሚቀር',\n",
       " 'ካሜራ',\n",
       " 'ባይጠመድም',\n",
       " 'ፍልሚያው',\n",
       " 'ይሄው',\n",
       " 'ተጧጡፎ',\n",
       " 'ቀጥሏል',\n",
       " '፡፡',\n",
       " 'በእነሱ',\n",
       " 'እንጂ',\n",
       " 'በእኛ',\n",
       " 'ቢግ',\n",
       " 'ብራዘር',\n",
       " 'ለመወዳደር',\n",
       " 'ምዝገባ',\n",
       " 'አያስፈልግም',\n",
       " '፡፡',\n",
       " 'ማጣሪያውን',\n",
       " 'ለማለፍ',\n",
       " 'የህዝብ',\n",
       " 'ድም',\n",
       " 'ወሳኝ',\n",
       " 'አይደለም',\n",
       " '፡፡',\n",
       " 'የ200',\n",
       " 'ሺህ',\n",
       " 'ዶላር',\n",
       " 'ሽልማት',\n",
       " 'ባገኝ',\n",
       " 'ብሎ',\n",
       " 'አይደለም',\n",
       " 'ኢትዮጵያዊ',\n",
       " 'ወደ',\n",
       " 'አገሩ',\n",
       " 'ቢግ',\n",
       " 'ብራዘር',\n",
       " 'የሚገባው',\n",
       " '፡፡',\n",
       " 'የደቡብ',\n",
       " 'አፍሪካውን',\n",
       " 'እንጂ',\n",
       " 'የእኛን',\n",
       " 'ቢግ',\n",
       " 'ብራዘር',\n",
       " 'የውድድር',\n",
       " 'ሂደት',\n",
       " 'የሚቀርፀው',\n",
       " 'ካሜራ',\n",
       " 'በተወሰነ',\n",
       " 'ቦታ',\n",
       " 'ላይ',\n",
       " 'አይተከልም',\n",
       " '፡፡',\n",
       " 'ደፋ',\n",
       " 'ቀና',\n",
       " 'ስንል',\n",
       " 'የሚያሳየንን',\n",
       " '፣',\n",
       " 'ቻይነታችንን',\n",
       " 'የሚያመለክተውን',\n",
       " 'የኑሮ',\n",
       " 'ፊልማችንን',\n",
       " 'የአለም',\n",
       " 'አይን',\n",
       " 'ለዘመናት',\n",
       " 'ደጋግሞ',\n",
       " 'ሲያየው',\n",
       " 'ኖሯል',\n",
       " 'እያየም',\n",
       " 'ሲያደንቀን',\n",
       " '፣',\n",
       " 'ሲንቀን',\n",
       " '፣',\n",
       " 'ሲስቅብን',\n",
       " '፣',\n",
       " 'ሲሳለቅብን',\n",
       " 'ሙድ',\n",
       " 'ሲይዝብን',\n",
       " '፡፡',\n",
       " 'ተሸነፋችሁ',\n",
       " 'የተባልንበት',\n",
       " 'አርቴፊሻል',\n",
       " 'ቢግ',\n",
       " 'ብራዘር',\n",
       " 'አፍሪካ',\n",
       " 'ከመጀመሩ',\n",
       " 'ከዘመናት',\n",
       " 'በፊት',\n",
       " 'እኮ',\n",
       " 'ነው',\n",
       " 'የእኛው',\n",
       " 'ብሔራዊ',\n",
       " 'ቢግ',\n",
       " 'ብራዘር',\n",
       " 'የተጀመረው',\n",
       " '፡፡',\n",
       " 'ከቢግ',\n",
       " 'ብራዘር',\n",
       " 'አፍሪካ',\n",
       " 'ቀድሞ',\n",
       " 'አለም',\n",
       " 'የእኛን',\n",
       " 'ቢግ',\n",
       " 'ብራዘር',\n",
       " 'ይከታተል',\n",
       " 'ነበር',\n",
       " '፡፡',\n",
       " 'አርቴፊሻል',\n",
       " 'ያልሆነውን',\n",
       " 'ፈተናችንን',\n",
       " '፣',\n",
       " 'ተሰልቶ',\n",
       " 'ያልተሰጠንን',\n",
       " 'መከራችንን',\n",
       " '፣',\n",
       " 'ህግ',\n",
       " 'ያልወጣለት',\n",
       " 'ገደብ',\n",
       " 'ያልተቀመጠለት',\n",
       " 'ከእዚህ',\n",
       " 'እስከ',\n",
       " 'እዚያ',\n",
       " 'ያልተባለለት',\n",
       " 'አብሮ',\n",
       " 'የመኖር',\n",
       " 'ትራጀዲያችንን',\n",
       " 'እያየ',\n",
       " 'አለም',\n",
       " 'ሁሉ',\n",
       " 'አጃኢብ',\n",
       " 'ሲል',\n",
       " 'ኖሯል',\n",
       " '፡፡',\n",
       " 'ተደናቂው',\n",
       " 'የእኛ',\n",
       " 'ቢግ',\n",
       " 'ብራዘር',\n",
       " 'በተወሰነ',\n",
       " 'ቻናል',\n",
       " 'ለተወሰነ',\n",
       " 'ተመልካች',\n",
       " 'አይደለም',\n",
       " 'ሲሰራጭ',\n",
       " 'የኖረው',\n",
       " '፡፡',\n",
       " 'አለም',\n",
       " 'ሁሉ',\n",
       " 'ሲያየው',\n",
       " 'ኖሯል',\n",
       " 'ዲኤስ',\n",
       " 'ቲቪ',\n",
       " 'ሳያስገጥም',\n",
       " '!',\n",
       " 'ያየውንም',\n",
       " 'ፎ',\n",
       " 'አስቀምጦታል',\n",
       " 'በየታሪክ',\n",
       " 'ድርሳኑ',\n",
       " 'በየ',\n",
       " 'መዝገበ',\n",
       " 'ቃላቱ',\n",
       " '፡፡',\n",
       " '(',\n",
       " 'በነገራችን',\n",
       " 'ላይ',\n",
       " ')',\n",
       " 'እነ',\n",
       " 'ሃኒ',\n",
       " 'ያሸንፉ',\n",
       " 'ዘንድ',\n",
       " 'የህዝብ',\n",
       " 'ድም',\n",
       " 'እንደ',\n",
       " 'ሁሉ',\n",
       " 'በኦክስፎርድ',\n",
       " 'ላይ',\n",
       " 'ያለውን',\n",
       " 'የሚል',\n",
       " 'ቃል',\n",
       " 'ፍቺ',\n",
       " 'ለማሠረዝም',\n",
       " 'የህዝብ',\n",
       " 'ድም',\n",
       " 'ዋጋ',\n",
       " 'የለውም',\n",
       " '!',\n",
       " '(',\n",
       " 'በፌስ',\n",
       " 'ቡክ',\n",
       " 'በኦክስፎርድ',\n",
       " 'መዝገበ',\n",
       " 'ቃላት',\n",
       " 'ላይ',\n",
       " 'ስለ',\n",
       " 'ኢትዮጵያ',\n",
       " 'የተፃፈውን',\n",
       " 'ፀያፍ',\n",
       " 'ነገር',\n",
       " 'ለማሰረዝ',\n",
       " 'ድም',\n",
       " 'እንስጥ',\n",
       " 'የሚል',\n",
       " 'ዘመቻ',\n",
       " 'መጀመሩን',\n",
       " 'ልብ',\n",
       " 'በሉ',\n",
       " ')',\n",
       " 'እና',\n",
       " 'ከኦክስፎርድ',\n",
       " 'ማሰረዝ',\n",
       " 'ቢቻል',\n",
       " 'እንኳን',\n",
       " 'ከአለም',\n",
       " 'ልቡና',\n",
       " 'ግን',\n",
       " 'መፋቅ',\n",
       " 'አይቻልም',\n",
       " '፡፡',\n",
       " 'እና',\n",
       " '!',\n",
       " 'ሃኒ',\n",
       " 'በ',\n",
       " 'ቢግ',\n",
       " 'ብራዘርስ',\n",
       " 'አፍሪካ',\n",
       " 'አልተሸነፈችም',\n",
       " '!',\n",
       " 'ምናልባት',\n",
       " 'ሃኒ',\n",
       " 'በ',\n",
       " 'ቢግ',\n",
       " 'ቢራዘርስ',\n",
       " 'አልቀረም',\n",
       " '፡፡',\n",
       " 'ሀኪሞቹ',\n",
       " 'ብልሀቱ',\n",
       " 'ገባቸው',\n",
       " '፡፡',\n",
       " 'ውሀ',\n",
       " 'ፈልቶ',\n",
       " 'በቂ',\n",
       " 'ይንተከተካል',\n",
       " '፡፡',\n",
       " '/',\n",
       " 'ውስጡ',\n",
       " 'ሊኖር',\n",
       " 'የሚችለው',\n",
       " 'በሽታ',\n",
       " 'እንዲጠፋ',\n",
       " '/',\n",
       " 'ውሀውን',\n",
       " 'በመርፌ',\n",
       " 'መድሀኒት',\n",
       " 'አስመስለው',\n",
       " 'ይወጉዋቸውና',\n",
       " '፣',\n",
       " 'ኋየታዘዘልህን',\n",
       " 'ኪኒን',\n",
       " 'እንደተባልከው',\n",
       " 'ካልዋጥክ',\n",
       " 'ግን',\n",
       " 'መርፌው',\n",
       " 'ብቻውን',\n",
       " 'አይሰራም',\n",
       " '፣',\n",
       " 'ይረክሳል',\n",
       " 'ይሉዋቸዋል',\n",
       " '፡፡',\n",
       " '/',\n",
       " 'ይሄ',\n",
       " '/',\n",
       " 'ስነ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 1000 tokens\n",
    "tokenized_data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove unnecessary characters from the tokenized data\n",
    "* **Purpose:** Simplify corpus, reduce noise, enhance generalization\n",
    "* **Approach:** Exclude punctuation marks to focus on essential content\n",
    "* **Considerations:** Task-specific requirements may warrant preserving certain punctuation **e.g: keep '::', '?'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T20:17:57.561141300Z",
     "start_time": "2023-11-08T20:17:56.444130300Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove punctuation marks from the tokenized data\n",
    "def remove_puncs(tokens: list):\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in puncs or token == '።' or token == '፡፡' or token == '?':\n",
    "            processed_tokens.append(token)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "processed_tokenized_data = remove_puncs(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Normalize the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the tokens by replacing vague character or sequence\n",
    "def normalize(norm: str):\n",
    "    replacements = {\"ሃ\": \"ሀ\", \"ኅ\": \"ሀ\", \"ኃ\": \"ሀ\", \"ሐ\": \"ሀ\", \"ሓ\": \"ሀ\", \"ኻ\": \"ሀ\", \"ሑ\": \"ሁ\", \n",
    "                    \"ኁ\": \"ሁ\", \"ዅ\": \"ሁ\", \"ኂ\": \"ሂ\", \"ሒ\": \"ሂ\", \"ኺ\": \"ሂ\", \"ኌ\": \"ሄ\", \"ሔ\": \"ሄ\", \n",
    "                    \"ዄ\": \"ሄ\", \"ሕ\": \"ህ\", \"ኆ\": \"ሆ\", \"ሖ\": \"ሆ\", \"ኾ\": \"ሆ\", \"ሠ\": \"ሰ\", \"ሡ\": \"ሱ\", \n",
    "                    \"ሢ\": \"ሲ\", \"ሣ\": \"ሳ\", \"ሤ\": \"ሴ\", \"ሥ\": \"ስ\", \"ሦ\": \"ሶ\", \"ዓ\": \"አ\", \"ኣ\": \"አ\", \n",
    "                    \"ዐ\": \"አ\", \"ዑ\": \"ኡ\", \"ዒ\": \"ኢ\", \"ዔ\": \"ኤ\", \"ዕ\": \"እ\", \"ዖ\": \"ኦ\", \"ፀ\": \"ጸ\", \n",
    "                    \"ፁ\": \"ጹ\", \"ጺ\": \"ፂ\", \"ጻ\": \"ፃ\", \"ጼ\": \"ፄ\", \"ፅ\": \"ጽ\", \"ፆ\": \"ጾ\"}\n",
    "\n",
    "    for character, replacement in replacements.items():\n",
    "        norm = norm.replace(character, replacement)\n",
    "\n",
    "    specific_patterns = [\n",
    "        '(ሉ[ዋአሃ])', '(ሙ[ዋአሃ])', '(ቱ[ዋአሃ])', '(ሩ[ዋአሃ])', '(ሱ[ዋአሃ])', '(ሹ[ዋአሃ])', '(ቁ[ዋአሃ])',\n",
    "        '(ቡ[ዋአሃ])', '(ቹ[ዋአሃ])', '(ሁ[ዋአሃ])', '(ኑ[ዋአሃ])', '(ኙ[ዋአሃ])', '(ኩ[ዋአሃ])', '(ዙ[ዋአሃ])',\n",
    "        '(ጉ[ዋአሃ])', '(ደ[ዋአሃ])', '(ጡ[ዋአሃ])', '(ጩ[ዋአሃ])', '(ጹ[ዋአሃ])', '(ፉ[ዋአሃ])', '[ቊ]', '[ኵ]',\n",
    "        '\\s+'\n",
    "    ]\n",
    "    replacements_specific = [\n",
    "        'ሏ', 'ሟ', 'ቷ', 'ሯ', 'ሷ', 'ሿ', 'ቋ',\n",
    "        'ቧ', 'ቿ', 'ኋ', 'ኗ', 'ኟ', 'ኳ', 'ዟ',\n",
    "        'ጓ', 'ዷ', 'ጧ', 'ጯ', 'ጿ', 'ፏ', 'ቁ', 'ኩ',\n",
    "        ' '\n",
    "    ]\n",
    "\n",
    "    for pattern, replacement in zip(specific_patterns, replacements_specific):\n",
    "        norm = re.sub(pattern, replacement, norm)\n",
    "\n",
    "\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = [normalize(token) for token in processed_tokenized_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_tokenized\t|normalized\n",
      "----------------------------------------\n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒም                       ሀኒም                      \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒም                       ሀኒም                      \n",
      "ሃኒም                       ሀኒም                      \n",
      "ለሦስት                      ለሶስት                     \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃትሪክ                      ሀትሪክ                     \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒን                       ሀኒን                      \n",
      "ያፀደቀለት                    ያጸደቀለት                   \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒ                        ሀኒ                       \n",
      "ተሸንፈሃል                    ተሸንፈሀል                   \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒ                        ሀኒ                       \n",
      "ከዓመት                      ከአመት                     \n",
      "የሚቀርፀው                    የሚቀርጸው                   \n",
      "ብሔራዊ                      ብሄራዊ                     \n",
      "ሃኒ                        ሀኒ                       \n",
      "ለማሠረዝም                    ለማሰረዝም                   \n",
      "ፀያፍ                       ጸያፍ                      \n",
      "ሃኒ                        ሀኒ                       \n",
      "ሃኒ                        ሀኒ                       \n",
      "ይወጉዋቸውና                   ይወጓቸውና                   \n",
      "ይሉዋቸዋል                    ይሏቸዋል                    \n"
     ]
    }
   ],
   "source": [
    "# print processed_tokenized_data vs normalized_data\n",
    "print(\"processed_tokenized\\t|normalized\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(1000):\n",
    "    if processed_tokenized_data[i] != normalized_data[i]:\n",
    "        print(\"{:25} {:25}\".format(processed_tokenized_data[i], normalized_data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1. N-gram language model\n",
    "### 1.1. Create n-grams for n=1, 2, 3, 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to create n-grams\n",
    "def create_n_grams(tokens: list, n: int):\n",
    "    n_grams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        n_grams.append(tokens[i:i + n])\n",
    "\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams (n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create n-grams for n=1\n",
    "unigrams = create_n_grams(normalized_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ምን'], ['መሰላችሁ'], ['?'], ['አንባቢያን'], ['ኢትዮጵያ']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 unigrams\n",
    "unigrams[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams (n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create n-grams for n=2\n",
    "bigrams = create_n_grams(normalized_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ምን', 'መሰላችሁ'],\n",
       " ['መሰላችሁ', '?'],\n",
       " ['?', 'አንባቢያን'],\n",
       " ['አንባቢያን', 'ኢትዮጵያ'],\n",
       " ['ኢትዮጵያ', 'በተደጋጋሚ']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 bigrams\n",
    "bigrams[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigrams (n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create n-grams for n=3\n",
    "trigrams = create_n_grams(normalized_data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ምን', 'መሰላችሁ', '?'],\n",
       " ['መሰላችሁ', '?', 'አንባቢያን'],\n",
       " ['?', 'አንባቢያን', 'ኢትዮጵያ'],\n",
       " ['አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ'],\n",
       " ['ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 trigrams\n",
    "trigrams[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourgrams (n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create n-grams for n=4\n",
    "fourgrams = create_n_grams(normalized_data, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ምን', 'መሰላችሁ', '?', 'አንባቢያን'],\n",
       " ['መሰላችሁ', '?', 'አንባቢያን', 'ኢትዮጵያ'],\n",
       " ['?', 'አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ'],\n",
       " ['አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው'],\n",
       " ['ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 fourgrams\n",
    "fourgrams[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n.\n",
    "#### Probability of n-grams (n=1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate n-gram probabilities for n=1\n",
    "def calculate_unigram_probabilities(unigrams: list):\n",
    "    unigram_counts = {}\n",
    "    for unigram in unigrams:\n",
    "        unigram_tuple = tuple(unigram)  # Convert list to tuple\n",
    "        if unigram_tuple in unigram_counts:\n",
    "            unigram_counts[unigram_tuple] += 1\n",
    "        else:\n",
    "            unigram_counts[unigram_tuple] = 1\n",
    "\n",
    "    unigram_probabilities = {}\n",
    "    total_unigrams = len(unigrams)\n",
    "    for unigram, count in unigram_counts.items():\n",
    "        unigram_probabilities[unigram] = count / total_unigrams\n",
    "\n",
    "    return unigram_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate unigram probabilities\n",
    "unigram_probabilities = list(calculate_unigram_probabilities(unigrams).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate n-gram probabilities for n=2\n",
    "def calculate_bigram_probabilities(bigrams: list):\n",
    "    bigram_counts = {}\n",
    "    for bigram in bigrams:\n",
    "        bigram_tuple = tuple(bigram)  # Convert list to tuple\n",
    "        if bigram_tuple in bigram_counts:\n",
    "            bigram_counts[bigram_tuple] += 1\n",
    "        else:\n",
    "            bigram_counts[bigram_tuple] = 1\n",
    "\n",
    "    bigram_probabilities = {}\n",
    "    total_bigrams = len(bigrams)\n",
    "    for bigram, count in bigram_counts.items():\n",
    "        bigram_probabilities[bigram] = count / total_bigrams\n",
    "\n",
    "    return bigram_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate bigram probabilities\n",
    "bigram_probabilities = list(calculate_bigram_probabilities(bigrams).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate n-gram probabilities for n=3\n",
    "def calculate_trigram_probabilities(trigrams: list):\n",
    "    trigram_counts = {}\n",
    "    for trigram in trigrams:\n",
    "        trigram_tuple = tuple(trigram)  # Convert list to tuple\n",
    "        if trigram_tuple in trigram_counts:\n",
    "            trigram_counts[trigram_tuple] += 1\n",
    "        else:\n",
    "            trigram_counts[trigram_tuple] = 1\n",
    "\n",
    "    trigram_probabilities = {}\n",
    "    total_trigrams = len(trigrams)\n",
    "    for trigram, count in trigram_counts.items():\n",
    "        trigram_probabilities[trigram] = count / total_trigrams\n",
    "\n",
    "    return trigram_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate trigram probabilities\n",
    "trigram_probabilities = list(calculate_trigram_probabilities(trigrams).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate n-gram probabilities for n=4\n",
    "def calculate_fourgram_probabilities(fourgrams: list):\n",
    "    fourgram_counts = {}\n",
    "    for fourgram in fourgrams:\n",
    "        fourgram_tuple = tuple(fourgram)  # Convert list to tuple\n",
    "        if fourgram_tuple in fourgram_counts:\n",
    "            fourgram_counts[fourgram_tuple] += 1\n",
    "        else:\n",
    "            fourgram_counts[fourgram_tuple] = 1\n",
    "\n",
    "    fourgram_probabilities = {}\n",
    "    total_fourgrams = len(fourgrams)\n",
    "    for fourgram, count in fourgram_counts.items():\n",
    "        fourgram_probabilities[fourgram] = count / total_fourgrams\n",
    "\n",
    "    return fourgram_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fourgram probabilities\n",
    "fourgram_probabilities = list(calculate_fourgram_probabilities(fourgrams).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram probabilities:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('ምን',), 0.0017841831978120792),\n",
       " (('መሰላችሁ',), 4.931475586740991e-05),\n",
       " (('?',), 0.005905496433072657),\n",
       " (('አንባቢያን',), 1.2008227703859256e-05),\n",
       " (('ኢትዮጵያ',), 0.0012402818308065628)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 probabilities for all n\n",
    "\n",
    "print(\"Unigram probabilities:\\n\")\n",
    "unigram_probabilities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram probabilities:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('ምን', 'መሰላችሁ'), 1.4729125397956964e-05),\n",
       " (('መሰላችሁ', '?'), 2.1755087513074367e-05),\n",
       " (('?', 'አንባቢያን'), 4.837151198015423e-08),\n",
       " (('አንባቢያን', 'ኢትዮጵያ'), 3.627863398511567e-08),\n",
       " (('ኢትዮጵያ', 'በተደጋጋሚ'), 6.167367777469664e-07)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Bigram probabilities:\\n\")\n",
    "bigram_probabilities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram probabilities:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('ምን', 'መሰላችሁ', '?'), 7.932928060677225e-06),\n",
       " (('መሰላችሁ', '?', 'አንባቢያን'), 1.2092878141276258e-08),\n",
       " (('?', 'አንባቢያን', 'ኢትዮጵያ'), 1.2092878141276258e-08),\n",
       " (('አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ'), 2.4185756282552517e-08),\n",
       " (('ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው'), 2.4185756282552517e-08)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Trigram probabilities:\\n\")\n",
    "trigram_probabilities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourgram probabilities:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('ምን', 'መሰላችሁ', '?', 'አንባቢያን'), 1.2092878287513962e-08),\n",
       " (('መሰላችሁ', '?', 'አንባቢያን', 'ኢትዮጵያ'), 1.2092878287513962e-08),\n",
       " (('?', 'አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ'), 1.2092878287513962e-08),\n",
       " (('አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው'), 2.4185756575027924e-08),\n",
       " (('ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት'), 2.4185756575027924e-08)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Fourgram probabilities:\\n\")\n",
    "fourgram_probabilities[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 most likely n-grams (n=1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most likely unigrams:\n",
      "።                              0.037662663416197846\n",
      "፡፡                             0.016365279499904357\n",
      "ነው                             0.014102677868638018\n",
      "ላይ                             0.009125249346071583\n",
      "?                              0.005905496433072657\n",
      "ውስጥ                           0.0041903635677772305\n",
      "ወደ                             0.004002416060251168\n",
      "እና                            0.0038550401579078315\n",
      "ጋር                             0.003525327843360276\n",
      "ነበር                           0.0033258195446107595\n"
     ]
    }
   ],
   "source": [
    "# find the top 10 most likely n-grams for n=1\n",
    "sorted_up = sorted(unigram_probabilities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 most likely unigrams:\")\n",
    "for i in range(10):\n",
    "    print(\"{:25} {:25}\".format(sorted_up[i][0][0], sorted_up[i][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most likely bigrams:\n",
      "ነው ።                            0.00538327766114936\n",
      "ነው ፡፡                          0.002615121145061073\n",
      "ነበር ።                         0.0016460704598066536\n",
      "? ?                           0.0012708647342545923\n",
      "አ ም                           0.0012682405797296687\n",
      "ናቸው ።                         0.0008217594312748502\n",
      "ነው ?                          0.0007919263012610901\n",
      "። ይህ                          0.0007865328776753028\n",
      "ነበር ፡፡                        0.0006848438666150236\n",
      "ነገር ግን                        0.0005411079187659953\n"
     ]
    }
   ],
   "source": [
    "# find the top 10 most likely n-grams for n=2\n",
    "sorted_bp = sorted(bigram_probabilities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 most likely bigrams:\")\n",
    "for i in range(10):\n",
    "    print(\"{:25} {:25}\".format(sorted_bp[i][0][0] + \" \" + sorted_bp[i][0][1], sorted_bp[i][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most likely trigrams:\n",
      "ነው ? ?                         0.000263781950895659\n",
      "። ነገር ግን                     0.00021120211673738984\n",
      "እ ኤ አ                         0.0002026161732570837\n",
      "ማለት ነው ።                     0.00018153828665683918\n",
      "። ይሁን እንጂ                    0.00014940750943546818\n",
      "ነው ። ይህ                      0.00014396571427189385\n",
      "እንዴት ነው ?                    0.00013570627850140217\n",
      "፡፡ ነገር ግን                     0.0001336021177048201\n",
      "ብቻ ነው ።                       0.0001221380692268902\n",
      "አ ም ኢሳት                      0.00011814741944026905\n"
     ]
    }
   ],
   "source": [
    "# find the top 10 most likely n-grams for n=3\n",
    "sorted_tp = sorted(trigram_probabilities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 most likely trigrams:\")\n",
    "for i in range(10):\n",
    "    print(\"{:25} {:25}\".format(sorted_tp[i][0][0] + \" \" + sorted_tp[i][0][1] + \" \" + sorted_tp[i][0][2], sorted_tp[i][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most likely fourgrams:\n",
      "አ ም ኢሳት ዜና                   0.00011705906182313514\n",
      "እንዴት ነው ? ?                  5.8735109842455314e-05\n",
      "? ? ? ?                       5.455097395497548e-05\n",
      "ቀን 2008 አ ም                  4.6714788824666435e-05\n",
      "ለምንድን ነው ? ?                 4.6412466867478585e-05\n",
      "ቀን 2007 አ ም                  4.0740906950634534e-05\n",
      "ቀን 2010 አ ም                   4.026928469742149e-05\n",
      "ቀን 2011 አ ም                   3.922929716469529e-05\n",
      "ምንድን ነው ? ?                  3.8419074319431854e-05\n",
      "ቀን 2012 አ ም                   3.488795385947778e-05\n"
     ]
    }
   ],
   "source": [
    "# find the top 10 most likely n-grams for n=4\n",
    "sorted_fp = sorted(fourgram_probabilities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 most likely fourgrams:\")\n",
    "for i in range(10):\n",
    "    print(\"{:25} {:25}\".format(sorted_fp[i][0][0] + \" \" + sorted_fp[i][0][1] + \" \" + sorted_fp[i][0][2] + \" \" + sorted_fp[i][0][3], sorted_fp[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Probability of a given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the probability of a given sentence\n",
    "def sentence_probability(sentence, n):\n",
    "    words = sentence.split()\n",
    "    total_probability = 1.0\n",
    "\n",
    "    if n == 1:\n",
    "        n_grams = unigram_probabilities\n",
    "        sentence_grams = [tuple([(words[i]) for i in range(len(words))])]\n",
    "    elif n == 2:\n",
    "        n_grams = bigram_probabilities\n",
    "        sentence_grams = [(words[i], words[i+1]) for i in range(len(words) - 1)]\n",
    "    elif n == 3:\n",
    "        n_grams = trigram_probabilities\n",
    "        sentence_grams = [(words[i], words[i+1], words[i+2]) for i in range(len(words) - 2)]\n",
    "    elif n == 4:\n",
    "        n_grams = fourgram_probabilities\n",
    "        sentence_grams = [(words[i], words[i+1], words[i+2], words[i+3]) for i in range(len(words) - 3)]\n",
    "    else:\n",
    "        raise ValueError(\"Sentence length must be between 1 and 4\")\n",
    "\n",
    "    for gram in sentence_grams:\n",
    "        found = False\n",
    "        for xgram, prob in n_grams:\n",
    "            if gram == xgram:\n",
    "                total_probability *= prob\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            total_probability *= 0.0  # Assume a default probability for unseen four-grams (smoothing)\n",
    "\n",
    "    return total_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of the sentence 'ኢትዮጵያ ታሪካዊ ሀገር ናት' is: 1.2092878287513962e-08\n"
     ]
    }
   ],
   "source": [
    "# calculate the probability of a sentence for sentence\n",
    "sent = \"ኢትዮጵያ ታሪካዊ ሀገር ናት\"\n",
    "\n",
    "probability = sentence_probability(sent, len(sent.split()))\n",
    "\n",
    "print(f\"The probability of the sentence '{sent}' is: {probability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Generate random sentences using the n-grams for n=1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random sentences using the n-grams for n=1, 2, 3, 4\n",
    "def generate_random_sentence(n):\n",
    "    if n == 1:\n",
    "        n_grams = unigram_probabilities\n",
    "    elif n == 2:\n",
    "        n_grams = bigram_probabilities\n",
    "    elif n == 3:\n",
    "        n_grams = trigram_probabilities\n",
    "    elif n == 4:\n",
    "        n_grams = fourgram_probabilities\n",
    "    else:\n",
    "        raise ValueError(\"Sentence length must be between 1 and 4\")\n",
    "\n",
    "    sentence = \"\"\n",
    "    # generating words until we get a sentence ending punctuation mark (።, ፡፡, ?)\n",
    "    while True:\n",
    "        if n == 1:\n",
    "            sentence += random.choices([x[0][0] for x in n_grams], [x[1] for x in n_grams])[0] + \" \"\n",
    "        elif n == 2:\n",
    "            sentence += random.choices([x[0][0] + \" \" + x[0][1] for x in n_grams], [x[1] for x in n_grams])[0] + \" \"\n",
    "        elif n == 3:\n",
    "            sentence += random.choices([x[0][0] + \" \" + x[0][1] + \" \" + x[0][2] for x in n_grams], [x[1] for x in n_grams])[0] + \" \"\n",
    "        elif n == 4:\n",
    "            sentence += random.choices([x[0][0] + \" \" + x[0][1] + \" \" + x[0][2] + \" \" + x[0][3] for x in n_grams], [x[1] for x in n_grams])[0] + \" \"\n",
    "\n",
    "        if sentence.startswith('። ') or sentence.startswith('፡፡ ') or sentence.startswith('? '):\n",
    "            sentence = sentence[2:]\n",
    "\n",
    "        if sentence.endswith('። ') or sentence.endswith('፡፡ ') or sentence.endswith('? '):\n",
    "            break\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \u001b[92m \n",
      "Unigram sentence: \n",
      " \u001b[00m አውቃለሁ አመታት የክርስቲያን እንቅስቃሴ ቀን መአዛ እየፈጠረ ክፍያ እየሄደች የገለጸውን እንዳለበት እስኪ ወይም የፖለቲካ ጎምቱ ተቋርጦ ይልቁንም ነበረች የለቀቁት የነፃነት ጀመር መስጠቱን ፓልሚራ ህሊናቸው በጐ አማራጭ ዛሬም አስከባሪ ባልተከፋፈለ የጊዜና ይገልፃሉ ጉዲት ለመግታት ብንጀምርስ ስደተኞች ህጉ ነን ታይቷል የአለም ። \n",
      "\u001b[1m \u001b[92m \n",
      "Bigram sentence: \n",
      " \u001b[00m አይታጠቡም ። \n",
      "\u001b[1m \u001b[92m \n",
      "Trigram sentence: \n",
      " \u001b[00m ጥቂት ቀናት አስቀድመው ይህ ተገቢውን ግንዛቤ በስተቀር ውጤቱ ገንዘብ ቤተሰቦቹ እንዳይጠይቁት መደረጉን ዉል በስራ መመሪያ ግን አሉበት ፡፡ \n",
      "\u001b[1m \u001b[92m \n",
      "Fourgram sentence: \n",
      " \u001b[00m ተግተው በሚሰሩት በእነዚህ ወንድሞች መብት ተሟጋች ድርጅቶች በኢትዮጵያ ወያላው ጀመረ ፡፡ ታዲያ በፍራንስ ፉትቦል መጽሄትና ድረገጾች ፈጣንና ቀለጣፋ ምላሽ መስጠት ኢትዮጵያን ከውድቀት ለመታደግም ሆነ ጉልህ ፋይዳ ይኖረዋል ። \n"
     ]
    }
   ],
   "source": [
    "# generate sentences for n_grams, n=1, 2, 3, 4\n",
    "unigram_sentence = generate_random_sentence(1)\n",
    "bigram_sentence = generate_random_sentence(2)\n",
    "trigram_sentence = generate_random_sentence(3)\n",
    "fourgram_sentence = generate_random_sentence(4)\n",
    "\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m {}\".format(\"\\nUnigram sentence: \\n\", unigram_sentence))\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m {}\".format(\"\\nBigram sentence: \\n\", bigram_sentence))\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m {}\".format(\"\\nTrigram sentence: \\n\", trigram_sentence))\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m {}\".format(\"\\nFourgram sentence: \\n\", fourgram_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "* **Unigram sentence:** The unigram sentence is not coherent and does not make sense. This is because the unigram model does not consider the context of the words. It only considers the probability of each word in the sentence. Therefore, the unigram model is not suitable for generating sentences.\n",
    "* **Bigram sentence:** The bigram sentence is more coherent than the unigram sentence. This is because the bigram model considers the probability of each word given the previous word. Therefore, the bigram model is more suitable for generating sentences than the unigram model.\n",
    "* **Trigram sentence:** The trigram sentence is more coherent than the bigram sentence. This is because the trigram model considers the probability of each word given the previous two words. Therefore, the trigram model is more suitable for generating sentences than the bigram model.\n",
    "* **Fourgram sentence:** The fourgram sentence is more coherent than the trigram sentence. This is because the fourgram model considers the probability of each word given the previous three words. Therefore, the fourgram model is more suitable for generating sentences than the trigram model.\n",
    "* **Conclusion:** The higher the n-gram model, the more coherent the generated sentence is. However, the higher the n-gram model, the more data is required to train the model. Therefore, the n-gram model should be chosen based on the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2. Evaluate these Language Models Using Intrinsic Evaluation Method\n",
    "#### Calculate perplexity for n_gram language models for n=1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate perplexity\n",
    "def calculate_perplexity(ngrams, total_words):\n",
    "\n",
    "    model_entropy = 0.0\n",
    "    for ngram, count in ngrams.items():\n",
    "        prob = count / total_words\n",
    "        model_entropy += -math.log2(prob)\n",
    "\n",
    "    model_entropy /= total_words\n",
    "    return math.pow(2, model_entropy)\n",
    "\n",
    "def evaluate_model(n, tokens):\n",
    "\n",
    "    ngrams = collections.Counter(tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1))\n",
    "    total_words = len(tokens)\n",
    "\n",
    "    return calculate_perplexity(ngrams, total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1, Perplexity: 1.466194395820576\n",
      "n = 2, Perplexity: 192.72106771103458\n",
      "n = 3, Perplexity: 39371.4665688541\n",
      "n = 4, Perplexity: 513550.1090238023\n"
     ]
    }
   ],
   "source": [
    "# Evaluating n-gram models for n = 1, 2, 3, 4\n",
    "perplexities = []\n",
    "for n in range(1, 5):\n",
    "    perplexity = evaluate_model(n, normalized_data)\n",
    "    perplexities.append(perplexity)\n",
    "\n",
    "# Printing the results\n",
    "for n, perplexity in enumerate(perplexities):\n",
    "    print(f\"n = {n + 1}, Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "* In general, higher n-gram models (such as trigrams, fourgrams) have higher perplexity compared to lower order n-grams (unigrams, bigrams) often indicate potential challenges or limitations in language model performance. Higher perplexity values imply higher uncertainty and poorer predictions made by the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3 Evaluate these Language Models Using Extrinsic Evaluation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a language model using the n-gram model for n=1, 2, 3, 4\n",
    "def create_language_model(n, tokens):\n",
    "    if n == 1:\n",
    "        n_grams = unigram_probabilities\n",
    "    elif n == 2:\n",
    "        n_grams = bigram_probabilities\n",
    "    elif n == 3:\n",
    "        n_grams = trigram_probabilities\n",
    "    elif n == 4:\n",
    "        n_grams = fourgram_probabilities\n",
    "    else:\n",
    "        raise ValueError(\"Sentence length must be between 1 and 4\")\n",
    "\n",
    "    language_model = {}\n",
    "    for ngram, prob in n_grams:\n",
    "        if n == 1:\n",
    "            language_model[ngram[0]] = prob\n",
    "        elif n == 2:\n",
    "            language_model[ngram[0] + \" \" + ngram[1]] = prob\n",
    "        elif n == 3:\n",
    "            language_model[ngram[0] + \" \" + ngram[1] + \" \" + ngram[2]] = prob\n",
    "        elif n == 4:\n",
    "            language_model[ngram[0] + \" \" + ngram[1] + \" \" + ngram[2] + \" \" + ngram[3]] = prob\n",
    "\n",
    "    return language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create language models for n=1, 2, 3, 4\n",
    "unigram_language_model = create_language_model(1, normalized_data)\n",
    "bigram_language_model = create_language_model(2, normalized_data)\n",
    "trigram_language_model = create_language_model(3, normalized_data)\n",
    "fourgram_language_model = create_language_model(4, normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_language_model(language_model, sentences):\n",
    "    total_probability = 0.0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "\n",
    "        for i in range(len(words)):\n",
    "            phrase = \" \".join(words[i - i:i + 1])\n",
    "\n",
    "            if phrase in language_model:\n",
    "                total_probability += language_model[phrase]\n",
    "            else:\n",
    "                total_probability += 0.0\n",
    "\n",
    "    return total_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው\"]\n",
    "unigram_probability = evaluate_language_model(unigram_language_model, sentences)\n",
    "bigram_probability = evaluate_language_model(bigram_language_model, sentences)\n",
    "trigram_probability = evaluate_language_model(trigram_language_model, sentences)\n",
    "fourgram_probability = evaluate_language_model(fourgram_language_model, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Unigram probability:  0.0012402818308065628\n",
      "Sentence Bigram probability:  6.167367777469664e-07\n",
      "Sentence Trigram probability:  2.4185756282552517e-08\n",
      "Sentence Fourgram probability:  2.4185756575027924e-08\n"
     ]
    }
   ],
   "source": [
    "# print the results\n",
    "print(\"Sentence Unigram probability: \", unigram_probability)\n",
    "print(\"Sentence Bigram probability: \", bigram_probability)\n",
    "print(\"Sentence Trigram probability: \", trigram_probability)\n",
    "print(\"Sentence Fourgram probability: \", fourgram_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time = 7552.461581230164 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Display total execution time of the entire program\n",
    "end = time.time()\n",
    "print(f\"Total execution time = {end - start} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "* In general the higher the n-gram model, the more coherent the generated sentence is. I calculated the probabilities of test sentences under each n-gram model. The assignment of probabilities varied depending on the sentence contents and how well it matched patterns in the training data. Some sentences may have received higher probabilities from higher n models, while others could be better predicted by lower n models. No definitive claim can be made that probabilities will consistently increase or decrease with n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "* I have dedicated considerable effort to completing this assignment, engaging in discussions with my peers, particularly Debela, to address the various aspects of the tasks. Our collaborative efforts have been beneficial in broadening our understanding and learning significantly throughout this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
