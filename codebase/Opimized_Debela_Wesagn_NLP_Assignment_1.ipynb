{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP) - Assignment 1\n",
    "## Instructor:\n",
    "* **Fantahun B. (Ph.D.)**\n",
    "### Student:\n",
    "* **Name:** Wesagn Dawit\n",
    "* **ID:** GSR/5257/15\n",
    "* **Program:** MSc in Artificial Intelligence\n",
    "#### Introduction:\n",
    "* This individual assignment is based on one version of the General Purpose Amharic Corpus (GPAC). The assignment involved building **n-gram language models for n values of n = 1,2,3,4** and evaluating their performance **intrinsically and extrinsically**. For intrinsic evaluation, **perplexity** was calculated by determining the probabilities of n-grams in the corpus and generating random sentences based on these n-grams. Since the corpus lacked labels, **text generation** served as the extrinsic evaluation method. Extrinsically, the models' ability to accurately assign probabilities to new sequences was assessed by calculating the likelihoods of test sentences under each model. Extrinsic evaluation utilizes the language models to generate text, with quality assessment based on probability. Higher probabilities indicate better generalization as the model was more likely to generate that sentence. Given an unlabeled corpus, text generation was employed to evaluate the n-gram models, creating random sentences for n=1 to 4. The generated sentences were evaluated based on their coherence and ability to make sense. The higher the n-gram model, the more coherent the generated sentence is. However, the higher the n-gram model, the more data is required to train the model. Therefore, the n-gram model should be chosen based on the available data and We think that is why our instructor specifically asked us to create n-gram models for n values of n = 1,2,3,4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:37:01.842738800Z",
     "start_time": "2023-12-10T04:37:00.975738300Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:38:10.367549900Z",
     "start_time": "2023-12-10T04:37:01.035736400Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the GPAC.txt file line by line and store the lines directly into a list.\n",
    "start = time.time()\n",
    "with open('GPAC.txt', 'r') as f:\n",
    "    data = f.readlines()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:38:10.387554100Z",
     "start_time": "2023-12-10T04:38:10.371550900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "     ምን መሰላችሁ? (አንባቢያን) ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው ያልቻለችው የአለም የእግር ኳስ ዋ ለ19ኛ ጊዜ በደቡብ አፍሪካ ሲጠጣ፣ በሩቅ እያየች አንጀቷ ባረረ ልክ በአመቱ በለስ ቀናትና ሌላ ዋ ልትታደም ሁለት ልጆቿን ወደ ደቡብ አፍሪካ ላከች፡፡6ኛው ቢግ ብራዘርስ አፍሪካ አብሮ የመኖር ውድድር በደቡብ አፍሪካ ተካሂዷል፡፡ ከተለያዩ 14 የአፍሪካ አገራት የተውጣጡ 26 ያህል ተሳታፊዎች የተካፈሉበት ይህ ውድድር፣ ግለሰቦች በፈታኝ ሁኔታ ውስጥ በማለፍ ብቃታቸውን የሚያስመሰክሩበት መሆኑን ሰምተናል፡፡ የሚገጥሟቸውን የተለያዩ ፈተናዎች በትእግስትና በጥበብ ማለፍ፣ ከሌሎች ጋር ተስማምቶ መዝለቅ፣ ችግሮችን በብልጠት መፍታት ወዘተ     በየጊዜው ከሚደረገው ቅነሳ ተርፈው ለ91 ቀናት ያህል በውድድሩ መቆየት የቻሉ ሁለት ተወዳዳሪዎች እያንዳንዳቸው 200 ሺህ ዶላር እንደሚ\n"
     ]
    }
   ],
   "source": [
    "# print sample 500 characters\n",
    "print(\"Sample data:\\n\", data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "#### 1. Tokenize the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:38:10.471548600Z",
     "start_time": "2023-12-10T04:38:10.389552600Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a list of punctuation marks\n",
    "special_chars = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "amharic_chars = ['፡፡', \"::\", '፡', '።', '፣', '፤', '፥', '፦', '፧', '፨']\n",
    "\n",
    "geez = ['፩', '፪', '፫', '፬', '፭', '፮', '፯', '፰', '፱', '፲', '፳', '፴', '፵', '፶', '፷', '፸', '፹', '፺', '፻']\n",
    "\n",
    "puncs = list(set(special_chars + amharic_chars + geez))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:38:10.520551100Z",
     "start_time": "2023-12-10T04:38:10.406552500Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to process the tokens\n",
    "def processed_list(split_list: list):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(split_list):\n",
    "        if (split_list[i] == ':' and i + 1 < len(split_list) and split_list[i + 1] == ':') or \\\n",
    "                (split_list[i] == '፡' and i + 1 < len(split_list) and split_list[i + 1] == '፡'):\n",
    "            tokens.append('።')\n",
    "            i += 2  # Skip the next character as it is part of a consecutive pair\n",
    "        else:\n",
    "            tokens.append(split_list[i])\n",
    "            i += 1\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:38:10.520551100Z",
     "start_time": "2023-12-10T04:38:10.417551400Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to tokenize the text data\n",
    "def amh_tokenizer(text: str):\n",
    "    # Escape punctuations to ensure they are not interpreted as regex operators\n",
    "    escaped_puncs = [re.escape(p) for p in puncs]\n",
    "\n",
    "    # Create the regex pattern: words or any of the specified punctuations\n",
    "    pattern = r'\\w+|' + '|'.join(escaped_puncs)\n",
    "\n",
    "    # Use re.findall to get all matches\n",
    "    words = re.findall(pattern, text)\n",
    "\n",
    "    # Remove empty strings and items with space only from the list\n",
    "    split_list = [word for word in words if word != '' and word != ' ']\n",
    "\n",
    "    return processed_list(split_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "395"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the original data\n",
    "tokenized_data = amh_tokenizer(data)\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T04:39:38.918839900Z",
     "start_time": "2023-12-10T04:38:10.430549800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  86949986\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens: \", len(tokenized_data))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T04:39:38.933956200Z",
     "start_time": "2023-12-10T04:39:38.921846200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:39:39.040949800Z",
     "start_time": "2023-12-10T04:39:38.936954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['ምን', 'መሰላችሁ', '?', '(', 'አንባቢያን', ')', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 10 tokens\n",
    "tokenized_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove unnecessary characters from the tokenized data\n",
    "* **Purpose:** Simplify corpus, reduce noise, enhance generalization\n",
    "* **Approach:** Exclude punctuation marks to focus on essential content\n",
    "* **Considerations:** Task-specific requirements may warrant preserving certain punctuation **e.g: keep '::', '?'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:40:51.162611600Z",
     "start_time": "2023-12-10T04:39:38.954955400Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove punctuation marks from the tokenized data\n",
    "def remove_puncs(tokens: list):\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in puncs or token == '።' or token == '፡፡' or token == '?':\n",
    "            processed_tokens.append(token)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "processed_tokenized_data = remove_puncs(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "16"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tokenized_data\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T04:40:52.853659500Z",
     "start_time": "2023-12-10T04:40:52.839659500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:40:52.961657200Z",
     "start_time": "2023-12-10T04:40:52.856660900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens after unecessary characters:  82693302\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens after unecessary characters: \", len(processed_tokenized_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Normalize the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T04:40:52.984663300Z",
     "start_time": "2023-12-10T04:40:52.877664500Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize the tokens by replacing vague character or sequence\n",
    "def normalize(norm: str):\n",
    "    replacements = {\"ሃ\": \"ሀ\", \"ኅ\": \"ሀ\", \"ኃ\": \"ሀ\", \"ሐ\": \"ሀ\", \"ሓ\": \"ሀ\", \"ኻ\": \"ሀ\", \"ሑ\": \"ሁ\",\n",
    "                    \"ኁ\": \"ሁ\", \"ዅ\": \"ሁ\", \"ኂ\": \"ሂ\", \"ሒ\": \"ሂ\", \"ኺ\": \"ሂ\", \"ኌ\": \"ሄ\", \"ሔ\": \"ሄ\",\n",
    "                    \"ዄ\": \"ሄ\", \"ሕ\": \"ህ\", \"ኆ\": \"ሆ\", \"ሖ\": \"ሆ\", \"ኾ\": \"ሆ\", \"ሠ\": \"ሰ\", \"ሡ\": \"ሱ\",\n",
    "                    \"ሢ\": \"ሲ\", \"ሣ\": \"ሳ\", \"ሤ\": \"ሴ\", \"ሥ\": \"ስ\", \"ሦ\": \"ሶ\", \"ዓ\": \"አ\", \"ኣ\": \"አ\",\n",
    "                    \"ዐ\": \"አ\", \"ዑ\": \"ኡ\", \"ዒ\": \"ኢ\", \"ዔ\": \"ኤ\", \"ዕ\": \"እ\", \"ዖ\": \"ኦ\", \"ፀ\": \"ጸ\",\n",
    "                    \"ፁ\": \"ጹ\", \"ጺ\": \"ፂ\", \"ጻ\": \"ፃ\", \"ጼ\": \"ፄ\", \"ፅ\": \"ጽ\", \"ፆ\": \"ጾ\"}\n",
    "\n",
    "    for character, replacement in replacements.items():\n",
    "        norm = norm.replace(character, replacement)\n",
    "\n",
    "    specific_patterns = [\n",
    "        '(ሉ[ዋአሃ])', '(ሙ[ዋአሃ])', '(ቱ[ዋአሃ])', '(ሩ[ዋአሃ])', '(ሱ[ዋአሃ])', '(ሹ[ዋአሃ])', '(ቁ[ዋአሃ])',\n",
    "        '(ቡ[ዋአሃ])', '(ቹ[ዋአሃ])', '(ሁ[ዋአሃ])', '(ኑ[ዋአሃ])', '(ኙ[ዋአሃ])', '(ኩ[ዋአሃ])', '(ዙ[ዋአሃ])',\n",
    "        '(ጉ[ዋአሃ])', '(ደ[ዋአሃ])', '(ጡ[ዋአሃ])', '(ጩ[ዋአሃ])', '(ጹ[ዋአሃ])', '(ፉ[ዋአሃ])', '[ቊ]', '[ኵ]',\n",
    "        '\\s+'\n",
    "    ]\n",
    "    replacements_specific = [\n",
    "        'ሏ', 'ሟ', 'ቷ', 'ሯ', 'ሷ', 'ሿ', 'ቋ',\n",
    "        'ቧ', 'ቿ', 'ኋ', 'ኗ', 'ኟ', 'ኳ', 'ዟ',\n",
    "        'ጓ', 'ዷ', 'ጧ', 'ጯ', 'ጿ', 'ፏ', 'ቁ', 'ኩ',\n",
    "        ' '\n",
    "    ]\n",
    "\n",
    "    for pattern, replacement in zip(specific_patterns, replacements_specific):\n",
    "        norm = re.sub(pattern, replacement, norm)\n",
    "\n",
    "\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# normalized_data = [normalize(token) for token in processed_tokenized_data]\n",
    "normalized_data = []\n",
    "\n",
    "for i in range(len(processed_tokenized_data)):\n",
    "    token = processed_tokenized_data[i]\n",
    "    if token in [\"።\", \"፡፡\"]:\n",
    "        normalized_data.append('።')\n",
    "    # For all other tokens, apply the normalize function\n",
    "    else:\n",
    "        normalized_data.append(normalize(token))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:42.434990300Z",
     "start_time": "2023-12-10T04:40:52.896661200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens after normalization:  82693302\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens after normalization: \", len(normalized_data))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:43.757203500Z",
     "start_time": "2023-12-10T05:13:43.750202Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:43.773203900Z",
     "start_time": "2023-12-10T05:13:43.761217600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_tokenized\t|normalized\n",
      "----------------------------------------\n",
      "፡፡                        ።                        \n",
      "፡፡                        ።                        \n",
      "፡፡                        ።                        \n"
     ]
    }
   ],
   "source": [
    "# print processed_tokenized_data vs normalized_data\n",
    "print(\"processed_tokenized\\t|normalized\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(100):\n",
    "    if processed_tokenized_data[i] != normalized_data[i]:\n",
    "        print(\"{:25} {:25}\".format(processed_tokenized_data[i], normalized_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "32"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del processed_tokenized_data\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:45.519261900Z",
     "start_time": "2023-12-10T05:13:43.774204700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1. N-gram language model\n",
    "### 1.1. Create n-grams for n=1, 2, 3, 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:45.594250100Z",
     "start_time": "2023-12-10T05:13:45.522257100Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to create n-grams\n",
    "def create_n_grams(tokens: list, n: int):\n",
    "\n",
    "    # filtered_tokens = [token for token in tokens if token not in ['<s>', '</s><s>', '</q><s>']]\n",
    "    filtered_tokens = [token for token in tokens]\n",
    "\n",
    "    # Now, create n-grams from the filtered_tokens list\n",
    "    return [filtered_tokens[i:i + n] for i in range(len(filtered_tokens) - n + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Unigrams (n=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:15:26.810640100Z",
     "start_time": "2023-12-10T05:13:45.539252200Z"
    }
   },
   "outputs": [],
   "source": [
    "# create n-grams for n=1\n",
    "unigrams = create_n_grams(normalized_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:15:26.827641700Z",
     "start_time": "2023-12-10T05:15:26.815641500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": "[['ምን'],\n ['መሰላችሁ'],\n ['አንባቢያን'],\n ['ኢትዮጵያ'],\n ['በተደጋጋሚ'],\n ['ጥሪው'],\n ['ደርሷት'],\n ['ልትታደመው'],\n ['ያልቻለችው'],\n ['የአለም']]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print 10 unigrams (skip punctuation marks)\n",
    "print(\"Unigrams:\")\n",
    "([unigram for unigram in unigrams[:100] if unigram[0] not in puncs][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bigrams (n=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:16:35.641969600Z",
     "start_time": "2023-12-10T05:15:26.887646200Z"
    }
   },
   "outputs": [],
   "source": [
    "# create n-grams for n=2\n",
    "bigrams = create_n_grams(normalized_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:16:35.657969Z",
     "start_time": "2023-12-10T05:16:35.646970400Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Print 10 bigrams (skip punctuation marks)\n",
    "# print(\"Bigrams:\")\n",
    "# ([bigram for bigram in bigrams[:100] if bigram[0] not in puncs and bigram[1] not in puncs][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Trigrams (n=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:19:06.755110100Z",
     "start_time": "2023-12-10T05:16:35.659974600Z"
    }
   },
   "outputs": [],
   "source": [
    "# create n-grams for n=3\n",
    "trigrams = create_n_grams(normalized_data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:19:06.934007Z",
     "start_time": "2023-12-10T05:19:06.768044800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": "[['አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ'],\n ['ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው'],\n ['በተደጋጋሚ', 'ጥሪው', 'ደርሷት'],\n ['ጥሪው', 'ደርሷት', 'ልትታደመው'],\n ['ደርሷት', 'ልትታደመው', 'ያልቻለችው'],\n ['ልትታደመው', 'ያልቻለችው', 'የአለም'],\n ['ያልቻለችው', 'የአለም', 'የእግር'],\n ['የአለም', 'የእግር', 'ኳስ'],\n ['የእግር', 'ኳስ', 'ዋ'],\n ['ኳስ', 'ዋ', 'ለ19ኛ']]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Print 10 trigrams (skip punctuation marks)\n",
    "print(\"Trigrams:\")\n",
    "([trigram for trigram in trigrams[:100] if trigram[0] not in puncs and trigram[1] not in puncs and trigram[2] not in puncs][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fourgrams (n=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:36:06.082421400Z",
     "start_time": "2023-12-10T05:19:06.812577700Z"
    }
   },
   "outputs": [],
   "source": [
    "# create n-grams for n=4\n",
    "fourgrams = create_n_grams(normalized_data, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:36:06.203411800Z",
     "start_time": "2023-12-10T05:36:06.101243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourgrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": "[['አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው'],\n ['ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት'],\n ['በተደጋጋሚ', 'ጥሪው', 'ደርሷት', 'ልትታደመው'],\n ['ጥሪው', 'ደርሷት', 'ልትታደመው', 'ያልቻለችው'],\n ['ደርሷት', 'ልትታደመው', 'ያልቻለችው', 'የአለም'],\n ['ልትታደመው', 'ያልቻለችው', 'የአለም', 'የእግር'],\n ['ያልቻለችው', 'የአለም', 'የእግር', 'ኳስ'],\n ['የአለም', 'የእግር', 'ኳስ', 'ዋ'],\n ['የእግር', 'ኳስ', 'ዋ', 'ለ19ኛ'],\n ['ኳስ', 'ዋ', 'ለ19ኛ', 'ጊዜ']]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Print 10 fourgrams (skip punctuation marks)\n",
    "print(\"Fourgrams:\")\n",
    "([fourgram for fourgram in fourgrams[:100] if fourgram[0] not in puncs and fourgram[1] not in puncs and fourgram[2] not in puncs and fourgram[3] not in puncs][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n.\n",
    "#### Probability of n-grams (n=1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def calculate_n_gram_probabilities(n_grams: list):\n",
    "    \"\"\"Calculate n-gram probabilities.\"\"\"\n",
    "    n_gram_counts = Counter(tuple(n_gram) for n_gram in n_grams)\n",
    "    total_n_grams = sum(n_gram_counts.values())\n",
    "\n",
    "    n_gram_probabilities = {n_gram: count / total_n_grams for n_gram, count in n_gram_counts.items()}\n",
    "    return n_gram_probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:36:06.218758900Z",
     "start_time": "2023-12-10T05:36:06.163988100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Calculate probabilities for each n-gram size\n",
    "unigram_probabilities = calculate_n_gram_probabilities(unigrams)\n",
    "bigram_probabilities = calculate_n_gram_probabilities(bigrams)\n",
    "trigram_probabilities = calculate_n_gram_probabilities(trigrams)\n",
    "fourgram_probabilities = calculate_n_gram_probabilities(fourgrams)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:46:44.814431800Z",
     "start_time": "2023-12-10T05:36:06.235766600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Top 10 most likely n-grams (n=1, 2, 3, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m \u001B[92m \n",
      "Top 5 most likely unigrams \u001B[00m\n",
      "----------------------------------------\n",
      "ነው                   0.01410268\n",
      "ላይ                   0.00912525\n",
      "ውስጥ                  0.00419036\n"
     ]
    }
   ],
   "source": [
    "# print the first 5 probabilities for all n\n",
    "\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m\".format(\"\\nTop 5 most likely unigrams\"))\n",
    "print(\"-\" * 40)\n",
    "for unigram, prob in sorted(unigram_probabilities.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    if unigram[0] not in puncs:\n",
    "        print(\"{:20} {:10.8f}\".format(unigram[0], prob))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:46:47.844137200Z",
     "start_time": "2023-12-10T05:46:45.929752500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m \u001B[92m \n",
      "Top 10 most likely bigrams \u001B[00m\n",
      "----------------------------------------\n",
      "አ ም                  0.00126824\n"
     ]
    }
   ],
   "source": [
    "# print top 10 most likely bi-grams\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m\".format(\"\\nTop 10 most likely bigrams\"))\n",
    "print(\"-\" * 40)\n",
    "for bigram, prob in sorted(bigram_probabilities.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    if bigram[0] not in puncs and bigram[1] not in puncs:\n",
    "        print(\"{:20} {:10.8f}\".format(bigram[0] + \" \" + bigram[1], prob))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:47:01.638472500Z",
     "start_time": "2023-12-10T05:46:56.404239100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m \u001B[92m \n",
      "Top 10 most likely trigrams \u001B[00m\n",
      "----------------------------------------\n",
      "እ ኤ አ                0.00020262\n"
     ]
    }
   ],
   "source": [
    "# print top 10 most likely tri-grams\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m\".format(\"\\nTop 10 most likely trigrams\"))\n",
    "print(\"-\" * 40)\n",
    "for trigram, prob in sorted(trigram_probabilities.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    if trigram[0] not in puncs and trigram[1] not in puncs and trigram[2] not in puncs:\n",
    "        print(\"{:20} {:10.8f}\".format(trigram[0] + \" \" + trigram[1] + \" \" + trigram[2], prob))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:47:34.348885800Z",
     "start_time": "2023-12-10T05:47:01.648205Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m \u001B[92m \n",
      "Top 10 most likely fourgrams \u001B[00m\n",
      "----------------------------------------\n",
      "አ ም ኢሳት ዜና           0.00011706\n",
      "ቀን 2008 አ ም          0.00004671\n",
      "ቀን 2007 አ ም          0.00004074\n",
      "ቀን 2010 አ ም          0.00004027\n",
      "ቀን 2011 አ ም          0.00003923\n"
     ]
    }
   ],
   "source": [
    "# print top 10 most likely four-grams\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m\".format(\"\\nTop 10 most likely fourgrams\"))\n",
    "print(\"-\" * 40)\n",
    "for fourgram, prob in sorted(fourgram_probabilities.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    if fourgram[0] not in puncs and fourgram[1] not in puncs and fourgram[2] not in puncs and fourgram[3] not in puncs:\n",
    "        print(\"{:20} {:10.8f}\".format(fourgram[0] + \" \" + fourgram[1] + \" \" + fourgram[2] + \" \" + fourgram[3], prob))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:47:47.837008900Z",
     "start_time": "2023-12-10T05:47:34.362885300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Probability of a given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "1.2092878287513962e-08"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the probability of a given sentence using four-grams\n",
    "sentence = normalize(\"ኢትዮጵያ ታሪካዊ ሀገር ናት\")\n",
    "fourgram_probabilities[tuple(sentence.split())]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:47:47.894123400Z",
     "start_time": "2023-12-10T05:47:47.846122600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:47:47.975629800Z",
     "start_time": "2023-12-10T05:47:47.905637500Z"
    }
   },
   "outputs": [],
   "source": [
    "# find the probability of a given sentence\n",
    "def sentence_probability(sentence, n, unigram_probabilities):\n",
    "    words = sentence.split()\n",
    "    total_probability = 1.0\n",
    "\n",
    "    if n == 1:\n",
    "        n_grams = unigram_probabilities\n",
    "        sentence_grams = [(words[i],) for i in range(len(words))]  # Single word tuples\n",
    "    elif n == 2:\n",
    "        n_grams = bigram_probabilities\n",
    "        sentence_grams = [(words[i], words[i+1]) for i in range(len(words) - 1)]\n",
    "    elif n == 3:\n",
    "        n_grams = trigram_probabilities\n",
    "        sentence_grams = [(words[i], words[i+1], words[i+2]) for i in range(len(words) - 2)]\n",
    "    elif n == 4:\n",
    "        n_grams = fourgram_probabilities\n",
    "        sentence_grams = [(words[i], words[i+1], words[i+2], words[i+3]) for i in range(len(words) - 3)]\n",
    "    else:\n",
    "        raise ValueError(\"n must be between 1 and 4\")\n",
    "\n",
    "    for gram in sentence_grams:\n",
    "        prob = n_grams.get(gram, 1e-10)  # Get the probability, default to 0.0000000001 if not found\n",
    "        total_probability *= prob\n",
    "\n",
    "    return total_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:47:47.991632900Z",
     "start_time": "2023-12-10T05:47:47.933631100Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the probability of a sentence for sentence\n",
    "sentence = \"ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው\" #ኢትዮጵያ ታሪካዊ ሀገር ናት\n",
    "uingtram_probability = sentence_probability(sentence, 1, unigram_probabilities)\n",
    "bigram_probability = sentence_probability(sentence, 2, bigram_probabilities)\n",
    "trigram_probability = sentence_probability(sentence, 3, trigram_probabilities)\n",
    "fourgram_probability = sentence_probability(sentence, 4, fourgram_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram probability of sentence 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው': 1.0478356707280566e-26\n",
      "Bigram probability of sentence 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው': 8.725269465276984e-30\n",
      "Trigram probability of sentence 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው': 1.414747765439349e-23\n",
      "Fourgram probability of sentence 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው': 5.849508211065065e-16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unigram probability of sentence '{sentence}': {uingtram_probability}\")\n",
    "print(f\"Bigram probability of sentence '{sentence}': {bigram_probability}\")\n",
    "print(f\"Trigram probability of sentence '{sentence}': {trigram_probability}\")\n",
    "print(f\"Fourgram probability of sentence '{sentence}': {fourgram_probability}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:47:47.993631100Z",
     "start_time": "2023-12-10T05:47:47.953635Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.725269465276984e-30, 1.0478356707280566e-26, 1.414747765439349e-23, 5.849508211065065e-16]\n"
     ]
    }
   ],
   "source": [
    "# sort\n",
    "sorted = [uingtram_probability, bigram_probability, trigram_probability, fourgram_probability]\n",
    "sorted.sort()\n",
    "print(sorted)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:47:47.994633200Z",
     "start_time": "2023-12-10T05:47:47.975629800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4. Generate random sentences using the n-grams for n=1, 2, 3, 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:47:48.100628700Z",
     "start_time": "2023-12-10T05:47:47.991632900Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate random sentences using the n-grams for n=1, 2, 3, 4\n",
    "def generate_random_sentence(n_gram_probabilities):\n",
    "    sentence_end_markers = ['።', '?']\n",
    "    sentence = []\n",
    "    non_marker_added = False  # Flag to check if at least one non-marker word has been added\n",
    "\n",
    "    while True:\n",
    "        word_tuple = random.choices(list(n_gram_probabilities.keys()), weights=n_gram_probabilities.values())[0]\n",
    "        joined_word = []\n",
    "        end_marker = \"\"\n",
    "        end_marker_flag = False\n",
    "\n",
    "        for word in word_tuple:\n",
    "\n",
    "            # if word in sentence_end_markers:\n",
    "            #     print(word)\n",
    "\n",
    "            if word in sentence_end_markers:\n",
    "                end_marker = word\n",
    "                end_marker_flag = True\n",
    "                break\n",
    "            else:\n",
    "                joined_word.append(word)\n",
    "                non_marker_added = True\n",
    "\n",
    "        if end_marker_flag:\n",
    "            sentence.append(' '.join(joined_word))\n",
    "            sentence.append(end_marker)\n",
    "            break\n",
    "        else:\n",
    "            sentence.append(' '.join(joined_word))\n",
    "\n",
    "    if not non_marker_added:\n",
    "        return generate_random_sentence(n_gram_probabilities)\n",
    "\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m \u001B[92m \n",
      "Unigram sentence: \n",
      " \u001B[00m ስእል በጎ አጠራጣሪ ውስጥ  ።\n",
      "\u001B[1m \u001B[92m \n",
      "Bigram sentence: \n",
      " \u001B[00m ስድስት የወርቅ አሉ ።\n",
      "\u001B[1m \u001B[92m \n",
      "Trigram sentence: \n",
      " \u001B[00m ምንጮች ያስረዳሉ ።\n",
      "\u001B[1m \u001B[92m \n",
      "Fourgram sentence: \n",
      " \u001B[00m ለማቋቋም አስበውና ቆርጠው የተነሱ ሙሉ ድጋፍ ይሻሉ እ በየስብሰባዎቹ በአብዛኛው የሚገኙት ሴቶች አይዋጥ አላቸው ።\n"
     ]
    }
   ],
   "source": [
    "generated_sentence = generate_random_sentence(unigram_probabilities)\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m {}\".format(\"\\nUnigram sentence: \\n\", generated_sentence))\n",
    "\n",
    "generated_sentence = generate_random_sentence(bigram_probabilities)\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m {}\".format(\"\\nBigram sentence: \\n\", generated_sentence))\n",
    "\n",
    "generated_sentence = generate_random_sentence(trigram_probabilities)\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m {}\".format(\"\\nTrigram sentence: \\n\", generated_sentence))\n",
    "\n",
    "generated_sentence = generate_random_sentence(fourgram_probabilities)\n",
    "print(\"\\033[1m \\033[92m {} \\033[00m {}\".format(\"\\nFourgram sentence: \\n\", generated_sentence))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:48:18.872957500Z",
     "start_time": "2023-12-10T05:47:48.043633100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "* **Unigram sentence:** The unigram sentence is not coherent and does not make sense. This is because the unigram model does not consider the context of the words. It only considers the probability of each word in the sentence. Therefore, the unigram model is not suitable for generating sentences.\n",
    "* **Bigram sentence:** The bigram sentence is more coherent than the unigram sentence. This is because the bigram model considers the probability of each word given the previous word. Therefore, the bigram model is more suitable for generating sentences than the unigram model.\n",
    "* **Trigram sentence:** The trigram sentence is more coherent than the bigram sentence. This is because the trigram model considers the probability of each word given the previous two words. Therefore, the trigram model is more suitable for generating sentences than the bigram model.\n",
    "* **Fourgram sentence:** The fourgram sentence is more coherent than the trigram sentence. This is because the fourgram model considers the probability of each word given the previous three words. Therefore, the fourgram model is more suitable for generating sentences than the trigram model.\n",
    "* **Conclusion:** The higher the n-gram model, the more coherent the generated sentence is. However, the higher the n-gram model, the more data is required to train the model. Therefore, the n-gram model should be chosen based on the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2. Evaluate these Language Models Using Intrinsic Evaluation Method\n",
    "#### Calculate perplexity for n_gram language models for n=1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:48:18.905962400Z",
     "start_time": "2023-12-10T05:48:18.874960700Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to calculate perplexity\n",
    "def calculate_perplexity(ngrams, total_words):\n",
    "\n",
    "    model_entropy = 0.0\n",
    "    for ngram, count in ngrams.items():\n",
    "        prob = count / total_words\n",
    "        model_entropy += -math.log2(prob)\n",
    "\n",
    "    model_entropy /= total_words\n",
    "    return math.pow(2, model_entropy)\n",
    "\n",
    "def evaluate_model(n, tokens):\n",
    "\n",
    "    ngrams = collections.Counter(tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1))\n",
    "    total_words = len(tokens)\n",
    "\n",
    "    return calculate_perplexity(ngrams, total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:53:36.265031900Z",
     "start_time": "2023-12-10T05:48:18.999107400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1, Perplexity: 1.4661943165044653\n",
      "n = 2, Perplexity: 186.01227838075573\n",
      "n = 3, Perplexity: 31872.187005459826\n",
      "n = 4, Perplexity: 386061.670096585\n"
     ]
    }
   ],
   "source": [
    "# Evaluating n-gram models for n = 1, 2, 3, 4\n",
    "perplexities = []\n",
    "for n in range(1, 5):\n",
    "    perplexity = evaluate_model(n, normalized_data)\n",
    "    perplexities.append(perplexity)\n",
    "\n",
    "# Printing the results\n",
    "for n, perplexity in enumerate(perplexities):\n",
    "    print(f\"n = {n + 1}, Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "* In general, higher n-gram models (such as trigrams, fourgrams) have higher perplexity compared to lower order n-grams (unigrams, bigrams) often indicate potential challenges or limitations in language model performance. Higher perplexity values imply higher uncertainty and poorer predictions made by the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3 Evaluate these Language Models Using Extrinsic Evaluation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:53:36.320800Z",
     "start_time": "2023-12-10T05:53:36.280134700Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_language_model(language_model, sentences, n):\n",
    "    total_probability = 1.0  # we start with a base probability of 1 as multiplying anything by 1 leaves it unchanged\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "\n",
    "        for i in range(n - 1, len(words)):\n",
    "            phrase = tuple(words[i - (n-1):i + 1])\n",
    "\n",
    "            if phrase in language_model:\n",
    "                total_probability *= language_model[phrase]\n",
    "            else:\n",
    "                total_probability *= 1e-6  # or some small probability for unseen n-grams\n",
    "\n",
    "    # return 0 if the total_probability is 1.0\n",
    "    return total_probability if total_probability != 1.0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:53:36.351343500Z",
     "start_time": "2023-12-10T05:53:36.304020600Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences1 = [\"ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው\", \"ኢትዮጵያ ታሪካዊ ሀገር ናት ሀገር ኢትዮጵያ\"]\n",
    "\n",
    "sentences2 = [\"ደርሷትልትታደመውጥሪው \", \"ሀገርኢትዮጵያ ናትታሪካዊ ሀገርኢትዮጵያ\", \"ኢትዮጵያታሪካዊ ሀገርናትየህዝብበተደጋጋሚ\"]\n",
    "\n",
    "# sentences = [\" \".join(normalized_data[-6:])]\n",
    "sentences = \"ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው\"\n",
    "\n",
    "# sentence = \"ወሳኝ መሆኑን የተገነዘበው ወዳጄ ነው እንግዲህ\"\n",
    "eval_unigram_probability = evaluate_language_model(unigram_probabilities, sentences, 1)\n",
    "eval_bigram_probability = evaluate_language_model(bigram_probabilities, sentences, 2)\n",
    "eval_trigram_probability = evaluate_language_model(trigram_probabilities, sentences, 3)\n",
    "eval_fourgram_probability = evaluate_language_model(fourgram_probabilities, sentences, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:53:36.418349Z",
     "start_time": "2023-12-10T05:53:36.339348800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The probability of sentences 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው' is to be generated by unigram model is: 1.8020207048523145e-107\n",
      "\n",
      "The probability of sentences 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው' is to be generated by bigram model is: 0\n",
      "\n",
      "The probability of sentences 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው' is to be generated by trigram model is: 0\n",
      "\n",
      "The probability of sentences 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው' is to be generated by fourgram model is: 0\n"
     ]
    }
   ],
   "source": [
    "# print the results\n",
    "print(\"\\nThe probability of sentences '{}' is to be generated by unigram model is: {}\".format(sentences, eval_unigram_probability))\n",
    "\n",
    "print(\"\\nThe probability of sentences '{}' is to be generated by bigram model is: {}\".format(sentences, eval_bigram_probability))\n",
    "#\n",
    "print(\"\\nThe probability of sentences '{}' is to be generated by trigram model is: {}\".format(sentences, eval_trigram_probability))\n",
    "\n",
    "print(\"\\nThe probability of sentences '{}' is to be generated by fourgram model is: {}\".format(sentences, eval_fourgram_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totol time of execution is 4594.860609292984 seconds.\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "\n",
    "print(\"Totol time of execution is {} seconds.\".format(end-start))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:53:36.419349400Z",
     "start_time": "2023-12-10T05:53:36.363349100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explanation:\n",
    "* In general the higher the n-gram model, the more coherent the generated sentence is. We calculated the probabilities of test sentences under each n-gram model. The assignment of probabilities varied depending on the sentence contents and how well it matched patterns in the training data. Some sentences may have received higher probabilities from higher n models, while others could be better predicted by lower n models. No definitive claim can be made that probabilities will consistently increase or decrease with n."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion:\n",
    "* We have dedicated considerable effort to completing this assignment, engaging in discussions with my peers, particularly Debela, to address the various aspects of the tasks. Our collaborative efforts have been beneficial in broadening our understanding and learning significantly throughout this process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T05:53:36.420349500Z",
     "start_time": "2023-12-10T05:53:36.376347400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
